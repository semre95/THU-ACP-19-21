{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sami/Desktop/homework-2/homework2-mlp/solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.6151\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 4.6021\t Accuracy 0.1573\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.5502\t Accuracy 0.2150\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.8354\t Accuracy 0.2755\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.4665\t Accuracy 0.3572\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.2397\t Accuracy 0.4194\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.0843\t Accuracy 0.4675\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.9710\t Accuracy 0.5049\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8844\t Accuracy 0.5361\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8157\t Accuracy 0.5620\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7595\t Accuracy 0.5847\n",
      "\n",
      "Epoch [0]\t Average training loss 0.7132\t Average training accuracy 0.6044\n",
      "Epoch [0]\t Average validation loss 0.2250\t Average validation accuracy 0.8366\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2182\t Accuracy 0.8400\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2337\t Accuracy 0.8127\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2302\t Accuracy 0.8171\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2301\t Accuracy 0.8147\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2271\t Accuracy 0.8184\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2241\t Accuracy 0.8220\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2216\t Accuracy 0.8263\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2196\t Accuracy 0.8287\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2173\t Accuracy 0.8323\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2151\t Accuracy 0.8348\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2129\t Accuracy 0.8373\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2104\t Average training accuracy 0.8397\n",
      "Epoch [1]\t Average validation loss 0.1675\t Average validation accuracy 0.8948\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1625\t Accuracy 0.8900\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1785\t Accuracy 0.8724\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1775\t Accuracy 0.8694\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1798\t Accuracy 0.8650\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1785\t Accuracy 0.8675\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1771\t Accuracy 0.8685\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1760\t Accuracy 0.8692\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1754\t Accuracy 0.8690\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1743\t Accuracy 0.8705\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1733\t Accuracy 0.8712\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1723\t Accuracy 0.8717\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1708\t Average training accuracy 0.8725\n",
      "Epoch [2]\t Average validation loss 0.1390\t Average validation accuracy 0.9108\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1372\t Accuracy 0.9300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1502\t Accuracy 0.8937\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1506\t Accuracy 0.8884\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1533\t Accuracy 0.8837\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1526\t Accuracy 0.8862\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1518\t Accuracy 0.8863\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1511\t Accuracy 0.8865\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1512\t Accuracy 0.8860\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1505\t Accuracy 0.8867\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1500\t Accuracy 0.8869\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1495\t Accuracy 0.8870\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1486\t Average training accuracy 0.8875\n",
      "Epoch [3]\t Average validation loss 0.1225\t Average validation accuracy 0.9190\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1218\t Accuracy 0.9400\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1332\t Accuracy 0.9057\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1343\t Accuracy 0.9002\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1372\t Accuracy 0.8954\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1367\t Accuracy 0.8973\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1362\t Accuracy 0.8972\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1358\t Accuracy 0.8970\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1361\t Accuracy 0.8959\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1357\t Accuracy 0.8961\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1355\t Accuracy 0.8963\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1353\t Accuracy 0.8961\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1346\t Average training accuracy 0.8965\n",
      "Epoch [4]\t Average validation loss 0.1120\t Average validation accuracy 0.9250\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1118\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1222\t Accuracy 0.9131\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1238\t Accuracy 0.9061\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1267\t Accuracy 0.9018\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1262\t Accuracy 0.9038\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1259\t Accuracy 0.9038\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1257\t Accuracy 0.9035\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1261\t Accuracy 0.9027\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1259\t Accuracy 0.9026\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1258\t Accuracy 0.9027\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1258\t Accuracy 0.9025\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1253\t Average training accuracy 0.9028\n",
      "Epoch [5]\t Average validation loss 0.1048\t Average validation accuracy 0.9284\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1048\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1147\t Accuracy 0.9175\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1164\t Accuracy 0.9112\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1193\t Accuracy 0.9068\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1189\t Accuracy 0.9088\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1187\t Accuracy 0.9088\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1186\t Accuracy 0.9087\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1190\t Accuracy 0.9075\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1189\t Accuracy 0.9075\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1189\t Accuracy 0.9075\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1190\t Accuracy 0.9072\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1186\t Average training accuracy 0.9073\n",
      "Epoch [6]\t Average validation loss 0.0995\t Average validation accuracy 0.9312\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0998\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1091\t Accuracy 0.9208\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1110\t Accuracy 0.9146\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1139\t Accuracy 0.9103\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1135\t Accuracy 0.9123\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1133\t Accuracy 0.9128\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1133\t Accuracy 0.9127\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1138\t Accuracy 0.9116\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1137\t Accuracy 0.9114\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1138\t Accuracy 0.9115\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1140\t Accuracy 0.9111\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1136\t Average training accuracy 0.9111\n",
      "Epoch [7]\t Average validation loss 0.0956\t Average validation accuracy 0.9346\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0959\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1049\t Accuracy 0.9229\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1068\t Accuracy 0.9172\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1097\t Accuracy 0.9135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1093\t Accuracy 0.9152\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1092\t Accuracy 0.9159\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1092\t Accuracy 0.9156\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1097\t Accuracy 0.9146\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1097\t Accuracy 0.9143\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1098\t Accuracy 0.9144\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1100\t Accuracy 0.9138\n",
      "\n",
      "Epoch [8]\t Average training loss 0.1098\t Average training accuracy 0.9137\n",
      "Epoch [8]\t Average validation loss 0.0924\t Average validation accuracy 0.9360\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0928\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1015\t Accuracy 0.9251\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1035\t Accuracy 0.9198\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1063\t Accuracy 0.9164\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1060\t Accuracy 0.9178\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1059\t Accuracy 0.9184\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1060\t Accuracy 0.9180\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1065\t Accuracy 0.9172\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1065\t Accuracy 0.9170\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1066\t Accuracy 0.9169\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1069\t Accuracy 0.9162\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1067\t Average training accuracy 0.9161\n",
      "Epoch [9]\t Average validation loss 0.0899\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0903\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0988\t Accuracy 0.9263\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1008\t Accuracy 0.9209\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1036\t Accuracy 0.9177\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1033\t Accuracy 0.9192\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1033\t Accuracy 0.9195\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1033\t Accuracy 0.9192\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1039\t Accuracy 0.9183\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1040\t Accuracy 0.9181\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1041\t Accuracy 0.9182\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1044\t Accuracy 0.9176\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1042\t Average training accuracy 0.9175\n",
      "Epoch [10]\t Average validation loss 0.0879\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0882\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0966\t Accuracy 0.9288\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0986\t Accuracy 0.9235\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1014\t Accuracy 0.9197\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1010\t Accuracy 0.9209\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1011\t Accuracy 0.9210\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1012\t Accuracy 0.9204\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1017\t Accuracy 0.9196\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1018\t Accuracy 0.9195\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1019\t Accuracy 0.9196\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1023\t Accuracy 0.9189\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1021\t Average training accuracy 0.9188\n",
      "Epoch [11]\t Average validation loss 0.0862\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0865\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0947\t Accuracy 0.9306\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0967\t Accuracy 0.9249\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0995\t Accuracy 0.9211\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0992\t Accuracy 0.9221\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0992\t Accuracy 0.9222\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0993\t Accuracy 0.9216\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0998\t Accuracy 0.9208\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.1000\t Accuracy 0.9207\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.1001\t Accuracy 0.9208\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1005\t Accuracy 0.9201\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1003\t Average training accuracy 0.9201\n",
      "Epoch [12]\t Average validation loss 0.0847\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0850\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0931\t Accuracy 0.9308\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0951\t Accuracy 0.9252\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0979\t Accuracy 0.9215\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0975\t Accuracy 0.9225\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0976\t Accuracy 0.9226\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0977\t Accuracy 0.9221\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0983\t Accuracy 0.9212\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0984\t Accuracy 0.9212\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0985\t Accuracy 0.9214\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0989\t Accuracy 0.9207\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0988\t Average training accuracy 0.9207\n",
      "Epoch [13]\t Average validation loss 0.0834\t Average validation accuracy 0.9420\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0837\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0917\t Accuracy 0.9314\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0937\t Accuracy 0.9260\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0965\t Accuracy 0.9225\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0961\t Accuracy 0.9232\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0962\t Accuracy 0.9234\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0963\t Accuracy 0.9230\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0969\t Accuracy 0.9221\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0970\t Accuracy 0.9221\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0972\t Accuracy 0.9222\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0976\t Accuracy 0.9215\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0974\t Average training accuracy 0.9216\n",
      "Epoch [14]\t Average validation loss 0.0823\t Average validation accuracy 0.9432\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0825\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0904\t Accuracy 0.9318\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0925\t Accuracy 0.9266\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0952\t Accuracy 0.9232\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0949\t Accuracy 0.9240\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0950\t Accuracy 0.9241\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0951\t Accuracy 0.9238\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0956\t Accuracy 0.9230\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0958\t Accuracy 0.9229\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0960\t Accuracy 0.9231\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0964\t Accuracy 0.9223\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0962\t Average training accuracy 0.9223\n",
      "Epoch [15]\t Average validation loss 0.0813\t Average validation accuracy 0.9434\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0815\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0893\t Accuracy 0.9316\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0914\t Accuracy 0.9269\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0941\t Accuracy 0.9237\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0938\t Accuracy 0.9246\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0939\t Accuracy 0.9249\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0940\t Accuracy 0.9246\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0945\t Accuracy 0.9238\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0947\t Accuracy 0.9237\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0949\t Accuracy 0.9238\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0953\t Accuracy 0.9231\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0952\t Average training accuracy 0.9230\n",
      "Epoch [16]\t Average validation loss 0.0805\t Average validation accuracy 0.9438\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0806\t Accuracy 0.9500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0883\t Accuracy 0.9320\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0904\t Accuracy 0.9273\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0931\t Accuracy 0.9240\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0928\t Accuracy 0.9249\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0929\t Accuracy 0.9254\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0930\t Accuracy 0.9252\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0936\t Accuracy 0.9244\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0938\t Accuracy 0.9243\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0939\t Accuracy 0.9245\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0944\t Accuracy 0.9237\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0942\t Average training accuracy 0.9235\n",
      "Epoch [17]\t Average validation loss 0.0797\t Average validation accuracy 0.9444\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0799\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0874\t Accuracy 0.9320\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0895\t Accuracy 0.9278\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0922\t Accuracy 0.9246\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0919\t Accuracy 0.9255\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0919\t Accuracy 0.9259\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0921\t Accuracy 0.9255\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0926\t Accuracy 0.9248\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0929\t Accuracy 0.9247\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0930\t Accuracy 0.9249\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0935\t Accuracy 0.9242\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0933\t Average training accuracy 0.9240\n",
      "Epoch [18]\t Average validation loss 0.0790\t Average validation accuracy 0.9454\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0792\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0866\t Accuracy 0.9318\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0887\t Accuracy 0.9280\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0913\t Accuracy 0.9248\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0910\t Accuracy 0.9258\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0911\t Accuracy 0.9262\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0913\t Accuracy 0.9258\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0918\t Accuracy 0.9251\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0920\t Accuracy 0.9250\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0922\t Accuracy 0.9253\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0927\t Accuracy 0.9245\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0925\t Average training accuracy 0.9244\n",
      "Epoch [19]\t Average validation loss 0.0784\t Average validation accuracy 0.9454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9296.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5550\t Accuracy 0.1200\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.5251\t Accuracy 0.5776\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.3910\t Accuracy 0.6829\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.3344\t Accuracy 0.7289\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.2980\t Accuracy 0.7594\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.2727\t Accuracy 0.7806\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.2536\t Accuracy 0.7957\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.2390\t Accuracy 0.8075\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.2268\t Accuracy 0.8170\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.2164\t Accuracy 0.8260\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.2084\t Accuracy 0.8317\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2007\t Average training accuracy 0.8379\n",
      "Epoch [0]\t Average validation loss 0.1061\t Average validation accuracy 0.9308\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.0996\t Accuracy 0.9300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1127\t Accuracy 0.9180\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1134\t Accuracy 0.9137\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1146\t Accuracy 0.9111\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1129\t Accuracy 0.9142\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1120\t Accuracy 0.9150\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1111\t Accuracy 0.9147\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1104\t Accuracy 0.9148\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1097\t Accuracy 0.9147\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1086\t Accuracy 0.9160\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1083\t Accuracy 0.9156\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1073\t Average training accuracy 0.9163\n",
      "Epoch [1]\t Average validation loss 0.0843\t Average validation accuracy 0.9440\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0795\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0895\t Accuracy 0.9341\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0916\t Accuracy 0.9303\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0934\t Accuracy 0.9283\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0925\t Accuracy 0.9304\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0922\t Accuracy 0.9303\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0920\t Accuracy 0.9303\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0919\t Accuracy 0.9301\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0918\t Accuracy 0.9300\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0913\t Accuracy 0.9310\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0915\t Accuracy 0.9305\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0908\t Average training accuracy 0.9308\n",
      "Epoch [2]\t Average validation loss 0.0749\t Average validation accuracy 0.9536\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0700\t Accuracy 0.9400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0789\t Accuracy 0.9424\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0814\t Accuracy 0.9381\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0833\t Accuracy 0.9365\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0826\t Accuracy 0.9382\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0824\t Accuracy 0.9384\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0824\t Accuracy 0.9382\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0825\t Accuracy 0.9378\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0825\t Accuracy 0.9378\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0821\t Accuracy 0.9387\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0825\t Accuracy 0.9381\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0820\t Average training accuracy 0.9383\n",
      "Epoch [3]\t Average validation loss 0.0695\t Average validation accuracy 0.9564\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0642\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0727\t Accuracy 0.9473\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0751\t Accuracy 0.9442\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0769\t Accuracy 0.9429\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0764\t Accuracy 0.9439\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0761\t Accuracy 0.9441\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0763\t Accuracy 0.9439\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0764\t Accuracy 0.9437\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0765\t Accuracy 0.9435\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0762\t Accuracy 0.9443\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0767\t Accuracy 0.9437\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0763\t Average training accuracy 0.9438\n",
      "Epoch [4]\t Average validation loss 0.0658\t Average validation accuracy 0.9596\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0606\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0684\t Accuracy 0.9524\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0708\t Accuracy 0.9477\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0724\t Accuracy 0.9472\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0720\t Accuracy 0.9479\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0718\t Accuracy 0.9481\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0720\t Accuracy 0.9476\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0721\t Accuracy 0.9477\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0722\t Accuracy 0.9475\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0720\t Accuracy 0.9481\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0725\t Accuracy 0.9474\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0722\t Average training accuracy 0.9476\n",
      "Epoch [5]\t Average validation loss 0.0631\t Average validation accuracy 0.9616\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0580\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0652\t Accuracy 0.9559\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0675\t Accuracy 0.9516\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0690\t Accuracy 0.9508\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0687\t Accuracy 0.9511\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0684\t Accuracy 0.9513\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0687\t Accuracy 0.9508\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0688\t Accuracy 0.9510\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0689\t Accuracy 0.9507\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0688\t Accuracy 0.9512\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0692\t Accuracy 0.9505\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0689\t Average training accuracy 0.9507\n",
      "Epoch [6]\t Average validation loss 0.0608\t Average validation accuracy 0.9640\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0562\t Accuracy 0.9700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0625\t Accuracy 0.9584\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0648\t Accuracy 0.9548\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0662\t Accuracy 0.9540\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0659\t Accuracy 0.9538\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0656\t Accuracy 0.9539\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0660\t Accuracy 0.9533\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0661\t Accuracy 0.9532\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0662\t Accuracy 0.9530\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0660\t Accuracy 0.9533\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0665\t Accuracy 0.9528\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0662\t Average training accuracy 0.9530\n",
      "Epoch [7]\t Average validation loss 0.0588\t Average validation accuracy 0.9654\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0545\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0603\t Accuracy 0.9604\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0624\t Accuracy 0.9577\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0638\t Accuracy 0.9567\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0635\t Accuracy 0.9565\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0633\t Accuracy 0.9564\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0636\t Accuracy 0.9555\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0637\t Accuracy 0.9555\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0638\t Accuracy 0.9552\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0637\t Accuracy 0.9554\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0641\t Accuracy 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0639\t Average training accuracy 0.9551\n",
      "Epoch [8]\t Average validation loss 0.0571\t Average validation accuracy 0.9662\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0532\t Accuracy 0.9700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0584\t Accuracy 0.9622\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0603\t Accuracy 0.9597\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0617\t Accuracy 0.9588\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0614\t Accuracy 0.9588\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0611\t Accuracy 0.9586\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0615\t Accuracy 0.9578\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0616\t Accuracy 0.9576\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0617\t Accuracy 0.9573\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0616\t Accuracy 0.9575\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0621\t Accuracy 0.9569\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0619\t Average training accuracy 0.9570\n",
      "Epoch [9]\t Average validation loss 0.0556\t Average validation accuracy 0.9676\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0519\t Accuracy 0.9700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0567\t Accuracy 0.9641\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0585\t Accuracy 0.9613\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0598\t Accuracy 0.9604\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0596\t Accuracy 0.9604\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0593\t Accuracy 0.9605\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0597\t Accuracy 0.9597\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0598\t Accuracy 0.9596\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0599\t Accuracy 0.9593\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0598\t Accuracy 0.9594\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0602\t Accuracy 0.9588\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0601\t Average training accuracy 0.9588\n",
      "Epoch [10]\t Average validation loss 0.0542\t Average validation accuracy 0.9688\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0507\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0552\t Accuracy 0.9649\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0569\t Accuracy 0.9629\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0582\t Accuracy 0.9620\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0579\t Accuracy 0.9620\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0577\t Accuracy 0.9621\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0581\t Accuracy 0.9612\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0582\t Accuracy 0.9611\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0583\t Accuracy 0.9608\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0582\t Accuracy 0.9609\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0587\t Accuracy 0.9603\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0585\t Average training accuracy 0.9604\n",
      "Epoch [11]\t Average validation loss 0.0531\t Average validation accuracy 0.9704\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0496\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0539\t Accuracy 0.9665\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0555\t Accuracy 0.9641\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0568\t Accuracy 0.9632\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0565\t Accuracy 0.9631\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0563\t Accuracy 0.9633\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0567\t Accuracy 0.9625\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0568\t Accuracy 0.9624\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0569\t Accuracy 0.9621\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0568\t Accuracy 0.9621\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0573\t Accuracy 0.9616\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0571\t Average training accuracy 0.9617\n",
      "Epoch [12]\t Average validation loss 0.0521\t Average validation accuracy 0.9712\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0488\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0527\t Accuracy 0.9675\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0543\t Accuracy 0.9650\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0555\t Accuracy 0.9643\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0553\t Accuracy 0.9645\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0550\t Accuracy 0.9648\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0555\t Accuracy 0.9639\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0556\t Accuracy 0.9639\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0556\t Accuracy 0.9636\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0556\t Accuracy 0.9636\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0560\t Accuracy 0.9630\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0559\t Average training accuracy 0.9630\n",
      "Epoch [13]\t Average validation loss 0.0513\t Average validation accuracy 0.9716\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0482\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0517\t Accuracy 0.9680\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0532\t Accuracy 0.9656\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0544\t Accuracy 0.9654\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0542\t Accuracy 0.9659\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0539\t Accuracy 0.9661\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0544\t Accuracy 0.9652\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0545\t Accuracy 0.9651\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0545\t Accuracy 0.9648\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0545\t Accuracy 0.9647\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0550\t Accuracy 0.9640\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0548\t Average training accuracy 0.9640\n",
      "Epoch [14]\t Average validation loss 0.0505\t Average validation accuracy 0.9724\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0476\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0508\t Accuracy 0.9682\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0522\t Accuracy 0.9663\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0534\t Accuracy 0.9660\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0532\t Accuracy 0.9666\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0530\t Accuracy 0.9669\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0534\t Accuracy 0.9660\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0535\t Accuracy 0.9660\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0536\t Accuracy 0.9657\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0535\t Accuracy 0.9656\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0540\t Accuracy 0.9649\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0539\t Average training accuracy 0.9649\n",
      "Epoch [15]\t Average validation loss 0.0499\t Average validation accuracy 0.9724\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0473\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0499\t Accuracy 0.9690\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0514\t Accuracy 0.9673\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0525\t Accuracy 0.9670\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0523\t Accuracy 0.9673\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0521\t Accuracy 0.9674\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0526\t Accuracy 0.9665\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0526\t Accuracy 0.9666\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0527\t Accuracy 0.9663\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0526\t Accuracy 0.9663\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0531\t Accuracy 0.9656\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0530\t Average training accuracy 0.9655\n",
      "Epoch [16]\t Average validation loss 0.0494\t Average validation accuracy 0.9738\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0471\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0492\t Accuracy 0.9698\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0506\t Accuracy 0.9680\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0517\t Accuracy 0.9681\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0515\t Accuracy 0.9683\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0513\t Accuracy 0.9682\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0518\t Accuracy 0.9673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0518\t Accuracy 0.9673\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0519\t Accuracy 0.9671\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0519\t Accuracy 0.9671\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0523\t Accuracy 0.9664\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0522\t Average training accuracy 0.9664\n",
      "Epoch [17]\t Average validation loss 0.0489\t Average validation accuracy 0.9740\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0466\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0485\t Accuracy 0.9712\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0499\t Accuracy 0.9691\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0510\t Accuracy 0.9691\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0508\t Accuracy 0.9692\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0506\t Accuracy 0.9689\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0511\t Accuracy 0.9680\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0511\t Accuracy 0.9680\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0512\t Accuracy 0.9678\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0511\t Accuracy 0.9678\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0516\t Accuracy 0.9671\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0515\t Average training accuracy 0.9671\n",
      "Epoch [18]\t Average validation loss 0.0484\t Average validation accuracy 0.9746\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0462\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0479\t Accuracy 0.9706\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0493\t Accuracy 0.9690\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0503\t Accuracy 0.9691\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0501\t Accuracy 0.9695\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0499\t Accuracy 0.9692\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0504\t Accuracy 0.9683\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0504\t Accuracy 0.9684\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0505\t Accuracy 0.9681\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0505\t Accuracy 0.9681\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0509\t Accuracy 0.9674\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0509\t Average training accuracy 0.9674\n",
      "Epoch [19]\t Average validation loss 0.0480\t Average validation accuracy 0.9748\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9656.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.5801\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.5388\t Accuracy 0.1020\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4858\t Accuracy 0.1044\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.4536\t Accuracy 0.1142\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.4279\t Accuracy 0.1239\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.4058\t Accuracy 0.1313\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.3842\t Accuracy 0.1407\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.3669\t Accuracy 0.1482\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.3517\t Accuracy 0.1547\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.3373\t Accuracy 0.1620\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.3239\t Accuracy 0.1679\n",
      "\n",
      "Epoch [0]\t Average training loss 2.3120\t Average training accuracy 0.1740\n",
      "Epoch [0]\t Average validation loss 2.1830\t Average validation accuracy 0.2482\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 2.1917\t Accuracy 0.2500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 2.1848\t Accuracy 0.2412\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 2.1752\t Accuracy 0.2501\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 2.1687\t Accuracy 0.2618\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 2.1639\t Accuracy 0.2697\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 2.1587\t Accuracy 0.2798\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 2.1521\t Accuracy 0.2893\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 2.1484\t Accuracy 0.2990\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 2.1441\t Accuracy 0.3085\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 2.1403\t Accuracy 0.3182\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 2.1357\t Accuracy 0.3287\n",
      "\n",
      "Epoch [1]\t Average training loss 2.1315\t Average training accuracy 0.3386\n",
      "Epoch [1]\t Average validation loss 2.0795\t Average validation accuracy 0.4634\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 2.0826\t Accuracy 0.4700\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 2.0838\t Accuracy 0.4535\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 2.0804\t Accuracy 0.4608\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 2.0769\t Accuracy 0.4698\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 2.0747\t Accuracy 0.4737\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 2.0712\t Accuracy 0.4797\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 2.0663\t Accuracy 0.4877\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 2.0647\t Accuracy 0.4911\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 2.0618\t Accuracy 0.4951\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 2.0596\t Accuracy 0.4994\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 2.0564\t Accuracy 0.5047\n",
      "\n",
      "Epoch [2]\t Average training loss 2.0534\t Average training accuracy 0.5101\n",
      "Epoch [2]\t Average validation loss 2.0110\t Average validation accuracy 0.5846\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 2.0118\t Accuracy 0.5200\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 2.0176\t Accuracy 0.5663\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 2.0157\t Accuracy 0.5679\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 2.0137\t Accuracy 0.5748\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 2.0126\t Accuracy 0.5770\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 2.0098\t Accuracy 0.5795\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 2.0057\t Accuracy 0.5847\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 2.0053\t Accuracy 0.5860\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 2.0031\t Accuracy 0.5877\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 2.0018\t Accuracy 0.5903\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.9993\t Accuracy 0.5933\n",
      "\n",
      "Epoch [3]\t Average training loss 1.9969\t Average training accuracy 0.5967\n",
      "Epoch [3]\t Average validation loss 1.9595\t Average validation accuracy 0.6548\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 1.9590\t Accuracy 0.6600\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.9681\t Accuracy 0.6308\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.9668\t Accuracy 0.6276\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.9659\t Accuracy 0.6330\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.9657\t Accuracy 0.6330\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.9635\t Accuracy 0.6345\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.9600\t Accuracy 0.6369\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.9604\t Accuracy 0.6372\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.9588\t Accuracy 0.6379\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.9581\t Accuracy 0.6388\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.9562\t Accuracy 0.6404\n",
      "\n",
      "Epoch [4]\t Average training loss 1.9543\t Average training accuracy 0.6426\n",
      "Epoch [4]\t Average validation loss 1.9206\t Average validation accuracy 0.6918\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 1.9191\t Accuracy 0.7100\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 1.9307\t Accuracy 0.6616\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 1.9300\t Accuracy 0.6591\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 1.9299\t Accuracy 0.6614\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 1.9303\t Accuracy 0.6618\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 1.9286\t Accuracy 0.6634\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 1.9256\t Accuracy 0.6652\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 1.9266\t Accuracy 0.6650\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 1.9254\t Accuracy 0.6655\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 1.9252\t Accuracy 0.6659\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 1.9238\t Accuracy 0.6671\n",
      "\n",
      "Epoch [5]\t Average training loss 1.9223\t Average training accuracy 0.6689\n",
      "Epoch [5]\t Average validation loss 1.8915\t Average validation accuracy 0.7094\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 1.8892\t Accuracy 0.7300\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 1.9028\t Accuracy 0.6796\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 1.9024\t Accuracy 0.6780\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 1.9030\t Accuracy 0.6796\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 1.9039\t Accuracy 0.6798\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 1.9025\t Accuracy 0.6810\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 1.8999\t Accuracy 0.6826\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 1.9015\t Accuracy 0.6825\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 1.9006\t Accuracy 0.6824\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 1.9007\t Accuracy 0.6824\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 1.8997\t Accuracy 0.6836\n",
      "\n",
      "Epoch [6]\t Average training loss 1.8985\t Average training accuracy 0.6851\n",
      "Epoch [6]\t Average validation loss 1.8701\t Average validation accuracy 0.7256\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 1.8671\t Accuracy 0.7400\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 1.8822\t Accuracy 0.6953\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 1.8820\t Accuracy 0.6928\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 1.8831\t Accuracy 0.6921\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 1.8845\t Accuracy 0.6921\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 1.8833\t Accuracy 0.6929\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 1.8810\t Accuracy 0.6945\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 1.8830\t Accuracy 0.6944\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 1.8824\t Accuracy 0.6943\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 1.8828\t Accuracy 0.6940\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 1.8820\t Accuracy 0.6950\n",
      "\n",
      "Epoch [7]\t Average training loss 1.8811\t Average training accuracy 0.6961\n",
      "Epoch [7]\t Average validation loss 1.8546\t Average validation accuracy 0.7356\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 1.8510\t Accuracy 0.7600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 1.8673\t Accuracy 0.7047\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 1.8673\t Accuracy 0.7011\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 1.8688\t Accuracy 0.6993\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 1.8706\t Accuracy 0.6994\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 1.8696\t Accuracy 0.7006\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 1.8676\t Accuracy 0.7018\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 1.8698\t Accuracy 0.7017\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 1.8694\t Accuracy 0.7016\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 1.8701\t Accuracy 0.7014\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 1.8695\t Accuracy 0.7022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 1.8688\t Average training accuracy 0.7033\n",
      "Epoch [8]\t Average validation loss 1.8439\t Average validation accuracy 0.7410\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 1.8396\t Accuracy 0.7500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 1.8570\t Accuracy 0.7098\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 1.8571\t Accuracy 0.7055\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 1.8589\t Accuracy 0.7038\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 1.8609\t Accuracy 0.7039\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 1.8601\t Accuracy 0.7049\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 1.8583\t Accuracy 0.7066\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 1.8608\t Accuracy 0.7066\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 1.8605\t Accuracy 0.7067\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 1.8613\t Accuracy 0.7065\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 1.8609\t Accuracy 0.7070\n",
      "\n",
      "Epoch [9]\t Average training loss 1.8604\t Average training accuracy 0.7080\n",
      "Epoch [9]\t Average validation loss 1.8368\t Average validation accuracy 0.7438\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 1.8320\t Accuracy 0.7500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 1.8501\t Accuracy 0.7131\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 1.8504\t Accuracy 0.7088\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 1.8525\t Accuracy 0.7064\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 1.8546\t Accuracy 0.7058\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 1.8539\t Accuracy 0.7071\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 1.8523\t Accuracy 0.7088\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 1.8550\t Accuracy 0.7087\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 1.8548\t Accuracy 0.7090\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 1.8558\t Accuracy 0.7086\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 1.8555\t Accuracy 0.7091\n",
      "\n",
      "Epoch [10]\t Average training loss 1.8551\t Average training accuracy 0.7104\n",
      "Epoch [10]\t Average validation loss 1.8327\t Average validation accuracy 0.7466\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 1.8273\t Accuracy 0.7500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 1.8461\t Accuracy 0.7133\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 1.8464\t Accuracy 0.7111\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 1.8487\t Accuracy 0.7086\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 1.8510\t Accuracy 0.7080\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 1.8504\t Accuracy 0.7089\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 1.8490\t Accuracy 0.7106\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 1.8518\t Accuracy 0.7101\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 1.8517\t Accuracy 0.7104\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 1.8528\t Accuracy 0.7100\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 1.8526\t Accuracy 0.7109\n",
      "\n",
      "Epoch [11]\t Average training loss 1.8523\t Average training accuracy 0.7122\n",
      "Epoch [11]\t Average validation loss 1.8309\t Average validation accuracy 0.7476\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 1.8250\t Accuracy 0.7500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 1.8443\t Accuracy 0.7149\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 1.8446\t Accuracy 0.7119\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 1.8471\t Accuracy 0.7089\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 1.8495\t Accuracy 0.7079\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 1.8490\t Accuracy 0.7089\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 1.8477\t Accuracy 0.7103\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 1.8505\t Accuracy 0.7095\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 1.8505\t Accuracy 0.7100\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 1.8517\t Accuracy 0.7098\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 1.8517\t Accuracy 0.7108\n",
      "\n",
      "Epoch [12]\t Average training loss 1.8515\t Average training accuracy 0.7121\n",
      "Epoch [12]\t Average validation loss 1.8308\t Average validation accuracy 0.7470\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 1.8245\t Accuracy 0.7400\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 1.8441\t Accuracy 0.7118\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 1.8446\t Accuracy 0.7083\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 1.8471\t Accuracy 0.7058\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 1.8496\t Accuracy 0.7052\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 1.8492\t Accuracy 0.7070\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 1.8480\t Accuracy 0.7085\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 1.8509\t Accuracy 0.7075\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 1.8509\t Accuracy 0.7084\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 1.8522\t Accuracy 0.7084\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 1.8522\t Accuracy 0.7096\n",
      "\n",
      "Epoch [13]\t Average training loss 1.8521\t Average training accuracy 0.7108\n",
      "Epoch [13]\t Average validation loss 1.8322\t Average validation accuracy 0.7456\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 1.8254\t Accuracy 0.7400\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 1.8454\t Accuracy 0.7127\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 1.8458\t Accuracy 0.7083\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 1.8484\t Accuracy 0.7054\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 1.8510\t Accuracy 0.7047\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 1.8506\t Accuracy 0.7061\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 1.8495\t Accuracy 0.7073\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 1.8525\t Accuracy 0.7062\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 1.8526\t Accuracy 0.7071\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 1.8539\t Accuracy 0.7073\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 1.8539\t Accuracy 0.7083\n",
      "\n",
      "Epoch [14]\t Average training loss 1.8539\t Average training accuracy 0.7094\n",
      "Epoch [14]\t Average validation loss 1.8347\t Average validation accuracy 0.7426\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 1.8275\t Accuracy 0.7400\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 1.8476\t Accuracy 0.7118\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 1.8481\t Accuracy 0.7068\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 1.8507\t Accuracy 0.7032\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 1.8534\t Accuracy 0.7028\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 1.8531\t Accuracy 0.7043\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 1.8520\t Accuracy 0.7054\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 1.8550\t Accuracy 0.7044\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 1.8551\t Accuracy 0.7053\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 1.8565\t Accuracy 0.7052\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 1.8566\t Accuracy 0.7061\n",
      "\n",
      "Epoch [15]\t Average training loss 1.8566\t Average training accuracy 0.7071\n",
      "Epoch [15]\t Average validation loss 1.8380\t Average validation accuracy 0.7384\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 1.8303\t Accuracy 0.7300\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 1.8507\t Accuracy 0.7100\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 1.8511\t Accuracy 0.7052\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 1.8538\t Accuracy 0.7016\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 1.8565\t Accuracy 0.7007\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 1.8562\t Accuracy 0.7020\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 1.8553\t Accuracy 0.7031\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 1.8583\t Accuracy 0.7020\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 1.8584\t Accuracy 0.7028\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 1.8597\t Accuracy 0.7028\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 1.8599\t Accuracy 0.7039\n",
      "\n",
      "Epoch [16]\t Average training loss 1.8599\t Average training accuracy 0.7048\n",
      "Epoch [16]\t Average validation loss 1.8419\t Average validation accuracy 0.7374\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 1.8339\t Accuracy 0.7300\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 1.8543\t Accuracy 0.7071\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 1.8548\t Accuracy 0.7022\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 1.8575\t Accuracy 0.6982\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 1.8602\t Accuracy 0.6972\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 1.8600\t Accuracy 0.6985\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 1.8591\t Accuracy 0.6992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 1.8621\t Accuracy 0.6981\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 1.8622\t Accuracy 0.6987\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 1.8636\t Accuracy 0.6989\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 1.8638\t Accuracy 0.7002\n",
      "\n",
      "Epoch [17]\t Average training loss 1.8638\t Average training accuracy 0.7012\n",
      "Epoch [17]\t Average validation loss 1.8462\t Average validation accuracy 0.7318\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 1.8379\t Accuracy 0.7300\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 1.8584\t Accuracy 0.7022\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 1.8589\t Accuracy 0.6983\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 1.8616\t Accuracy 0.6947\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 1.8643\t Accuracy 0.6933\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 1.8641\t Accuracy 0.6948\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 1.8633\t Accuracy 0.6953\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 1.8663\t Accuracy 0.6945\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 1.8664\t Accuracy 0.6950\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 1.8678\t Accuracy 0.6952\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 1.8680\t Accuracy 0.6963\n",
      "\n",
      "Epoch [18]\t Average training loss 1.8681\t Average training accuracy 0.6972\n",
      "Epoch [18]\t Average validation loss 1.8509\t Average validation accuracy 0.7266\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 1.8422\t Accuracy 0.7200\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 1.8628\t Accuracy 0.6969\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 1.8633\t Accuracy 0.6937\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 1.8660\t Accuracy 0.6897\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 1.8688\t Accuracy 0.6886\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 1.8685\t Accuracy 0.6900\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 1.8677\t Accuracy 0.6905\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 1.8707\t Accuracy 0.6898\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 1.8709\t Accuracy 0.6904\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 1.8722\t Accuracy 0.6906\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 1.8725\t Accuracy 0.6918\n",
      "\n",
      "Epoch [19]\t Average training loss 1.8726\t Average training accuracy 0.6926\n",
      "Epoch [19]\t Average validation loss 1.8559\t Average validation accuracy 0.7236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.7050.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7358\t Accuracy 0.0800\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.5296\t Accuracy 0.1049\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.4575\t Accuracy 0.1134\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.3856\t Accuracy 0.1319\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.3245\t Accuracy 0.1509\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2716\t Accuracy 0.1748\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.2211\t Accuracy 0.2006\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.1816\t Accuracy 0.2232\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.1419\t Accuracy 0.2472\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.1051\t Accuracy 0.2708\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.0699\t Accuracy 0.2914\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0361\t Average training accuracy 0.3125\n",
      "Epoch [0]\t Average validation loss 1.6281\t Average validation accuracy 0.5600\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.6847\t Accuracy 0.5300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.6287\t Accuracy 0.5665\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.6110\t Accuracy 0.5749\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.5965\t Accuracy 0.5789\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.5827\t Accuracy 0.5851\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.5652\t Accuracy 0.5908\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.5465\t Accuracy 0.5974\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.5374\t Accuracy 0.6014\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.5222\t Accuracy 0.6075\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.5087\t Accuracy 0.6137\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.4947\t Accuracy 0.6183\n",
      "\n",
      "Epoch [1]\t Average training loss 1.4799\t Average training accuracy 0.6249\n",
      "Epoch [1]\t Average validation loss 1.2652\t Average validation accuracy 0.7200\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.3085\t Accuracy 0.7000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.2878\t Accuracy 0.7059\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.2798\t Accuracy 0.7069\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.2770\t Accuracy 0.7042\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.2729\t Accuracy 0.7041\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.2633\t Accuracy 0.7061\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.2534\t Accuracy 0.7079\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.2525\t Accuracy 0.7076\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.2444\t Accuracy 0.7100\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.2381\t Accuracy 0.7124\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.2309\t Accuracy 0.7145\n",
      "\n",
      "Epoch [2]\t Average training loss 1.2225\t Average training accuracy 0.7177\n",
      "Epoch [2]\t Average validation loss 1.0710\t Average validation accuracy 0.7830\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.1110\t Accuracy 0.7800\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.1050\t Accuracy 0.7575\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.1024\t Accuracy 0.7589\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.1045\t Accuracy 0.7549\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.1042\t Accuracy 0.7546\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.0981\t Accuracy 0.7552\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.0921\t Accuracy 0.7560\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.0948\t Accuracy 0.7552\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.0899\t Accuracy 0.7568\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.0869\t Accuracy 0.7577\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.0829\t Accuracy 0.7585\n",
      "\n",
      "Epoch [3]\t Average training loss 1.0775\t Average training accuracy 0.7605\n",
      "Epoch [3]\t Average validation loss 0.9554\t Average validation accuracy 0.8250\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.9954\t Accuracy 0.8100\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.9958\t Accuracy 0.7849\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.9963\t Accuracy 0.7878\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.0010\t Accuracy 0.7842\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.0024\t Accuracy 0.7847\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.9980\t Accuracy 0.7849\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.9941\t Accuracy 0.7853\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.9984\t Accuracy 0.7836\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.9953\t Accuracy 0.7847\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.9940\t Accuracy 0.7855\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.9918\t Accuracy 0.7856\n",
      "\n",
      "Epoch [4]\t Average training loss 0.9880\t Average training accuracy 0.7873\n",
      "Epoch [4]\t Average validation loss 0.8818\t Average validation accuracy 0.8500\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.9228\t Accuracy 0.8300\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.9262\t Accuracy 0.8051\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.9284\t Accuracy 0.8072\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.9345\t Accuracy 0.8034\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.9367\t Accuracy 0.8039\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.9333\t Accuracy 0.8041\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.9306\t Accuracy 0.8044\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.9358\t Accuracy 0.8032\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.9337\t Accuracy 0.8039\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.9334\t Accuracy 0.8045\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.9322\t Accuracy 0.8045\n",
      "\n",
      "Epoch [5]\t Average training loss 0.9294\t Average training accuracy 0.8057\n",
      "Epoch [5]\t Average validation loss 0.8328\t Average validation accuracy 0.8618\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8750\t Accuracy 0.8400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8796\t Accuracy 0.8202\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8830\t Accuracy 0.8214\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8898\t Accuracy 0.8172\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8926\t Accuracy 0.8181\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8897\t Accuracy 0.8185\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8877\t Accuracy 0.8185\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8935\t Accuracy 0.8175\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8919\t Accuracy 0.8183\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8924\t Accuracy 0.8186\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8918\t Accuracy 0.8184\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8896\t Average training accuracy 0.8189\n",
      "Epoch [6]\t Average validation loss 0.7994\t Average validation accuracy 0.8714\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8426\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8477\t Accuracy 0.8337\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8517\t Accuracy 0.8328\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8591\t Accuracy 0.8289\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8622\t Accuracy 0.8296\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8597\t Accuracy 0.8298\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8583\t Accuracy 0.8294\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8643\t Accuracy 0.8281\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8632\t Accuracy 0.8286\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8641\t Accuracy 0.8286\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8639\t Accuracy 0.8281\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8622\t Average training accuracy 0.8284\n",
      "Epoch [7]\t Average validation loss 0.7765\t Average validation accuracy 0.8768\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8205\t Accuracy 0.8600\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8257\t Accuracy 0.8418\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8302\t Accuracy 0.8402\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8380\t Accuracy 0.8356\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8412\t Accuracy 0.8369\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8390\t Accuracy 0.8372\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8379\t Accuracy 0.8369\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8441\t Accuracy 0.8354\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8433\t Accuracy 0.8359\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8445\t Accuracy 0.8360\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8447\t Accuracy 0.8355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.8433\t Average training accuracy 0.8358\n",
      "Epoch [8]\t Average validation loss 0.7609\t Average validation accuracy 0.8810\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8049\t Accuracy 0.8700\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8106\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8154\t Accuracy 0.8455\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8235\t Accuracy 0.8409\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8269\t Accuracy 0.8422\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8248\t Accuracy 0.8425\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8239\t Accuracy 0.8423\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8302\t Accuracy 0.8410\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8297\t Accuracy 0.8416\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8311\t Accuracy 0.8415\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8315\t Accuracy 0.8411\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8303\t Average training accuracy 0.8411\n",
      "Epoch [9]\t Average validation loss 0.7504\t Average validation accuracy 0.8836\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7937\t Accuracy 0.8700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8004\t Accuracy 0.8522\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8054\t Accuracy 0.8501\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8136\t Accuracy 0.8451\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8171\t Accuracy 0.8463\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8152\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8145\t Accuracy 0.8466\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8208\t Accuracy 0.8455\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8205\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8221\t Accuracy 0.8461\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8226\t Accuracy 0.8455\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8216\t Average training accuracy 0.8455\n",
      "Epoch [10]\t Average validation loss 0.7436\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.7858\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.7937\t Accuracy 0.8559\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.7989\t Accuracy 0.8537\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8072\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8108\t Accuracy 0.8493\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8089\t Accuracy 0.8495\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8084\t Accuracy 0.8496\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8147\t Accuracy 0.8482\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8145\t Accuracy 0.8489\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8162\t Accuracy 0.8488\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8168\t Accuracy 0.8484\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8160\t Average training accuracy 0.8484\n",
      "Epoch [11]\t Average validation loss 0.7395\t Average validation accuracy 0.8892\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.7803\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.7896\t Accuracy 0.8594\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.7950\t Accuracy 0.8570\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8033\t Accuracy 0.8508\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8069\t Accuracy 0.8517\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8051\t Accuracy 0.8518\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8047\t Accuracy 0.8519\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8111\t Accuracy 0.8505\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8109\t Accuracy 0.8513\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8127\t Accuracy 0.8512\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8134\t Accuracy 0.8507\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8126\t Average training accuracy 0.8507\n",
      "Epoch [12]\t Average validation loss 0.7373\t Average validation accuracy 0.8902\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7772\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.7875\t Accuracy 0.8610\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.7929\t Accuracy 0.8582\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8013\t Accuracy 0.8526\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8049\t Accuracy 0.8536\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8031\t Accuracy 0.8535\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8028\t Accuracy 0.8540\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8091\t Accuracy 0.8525\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8091\t Accuracy 0.8531\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8109\t Accuracy 0.8531\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8117\t Accuracy 0.8526\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8110\t Average training accuracy 0.8527\n",
      "Epoch [13]\t Average validation loss 0.7366\t Average validation accuracy 0.8924\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7755\t Accuracy 0.8900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7868\t Accuracy 0.8637\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.7921\t Accuracy 0.8607\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8006\t Accuracy 0.8550\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8042\t Accuracy 0.8557\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8025\t Accuracy 0.8556\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8022\t Accuracy 0.8559\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8085\t Accuracy 0.8544\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8085\t Accuracy 0.8551\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8103\t Accuracy 0.8550\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8112\t Accuracy 0.8545\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8105\t Average training accuracy 0.8546\n",
      "Epoch [14]\t Average validation loss 0.7370\t Average validation accuracy 0.8946\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.7751\t Accuracy 0.8900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.7870\t Accuracy 0.8655\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.7924\t Accuracy 0.8623\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8008\t Accuracy 0.8566\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8045\t Accuracy 0.8570\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8028\t Accuracy 0.8569\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8025\t Accuracy 0.8573\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8088\t Accuracy 0.8560\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8089\t Accuracy 0.8566\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8107\t Accuracy 0.8565\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8116\t Accuracy 0.8559\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8110\t Average training accuracy 0.8559\n",
      "Epoch [15]\t Average validation loss 0.7381\t Average validation accuracy 0.8954\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.7755\t Accuracy 0.8900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.7880\t Accuracy 0.8667\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.7933\t Accuracy 0.8632\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8017\t Accuracy 0.8576\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8055\t Accuracy 0.8581\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8037\t Accuracy 0.8578\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8035\t Accuracy 0.8584\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8098\t Accuracy 0.8572\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8099\t Accuracy 0.8578\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8117\t Accuracy 0.8576\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8126\t Accuracy 0.8569\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8121\t Average training accuracy 0.8569\n",
      "Epoch [16]\t Average validation loss 0.7398\t Average validation accuracy 0.8966\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.7764\t Accuracy 0.8900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.7895\t Accuracy 0.8665\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.7948\t Accuracy 0.8637\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8032\t Accuracy 0.8581\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8069\t Accuracy 0.8585\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8052\t Accuracy 0.8583\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8050\t Accuracy 0.8589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8113\t Accuracy 0.8578\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8114\t Accuracy 0.8586\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8132\t Accuracy 0.8582\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8141\t Accuracy 0.8576\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8136\t Average training accuracy 0.8575\n",
      "Epoch [17]\t Average validation loss 0.7418\t Average validation accuracy 0.8972\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.7778\t Accuracy 0.8900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.7913\t Accuracy 0.8684\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.7966\t Accuracy 0.8656\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8049\t Accuracy 0.8593\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8087\t Accuracy 0.8594\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8070\t Accuracy 0.8592\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8068\t Accuracy 0.8597\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8131\t Accuracy 0.8585\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8132\t Accuracy 0.8592\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8151\t Accuracy 0.8589\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8160\t Accuracy 0.8582\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8154\t Average training accuracy 0.8581\n",
      "Epoch [18]\t Average validation loss 0.7440\t Average validation accuracy 0.8986\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.7795\t Accuracy 0.9000\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.7934\t Accuracy 0.8692\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.7986\t Accuracy 0.8663\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8069\t Accuracy 0.8601\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8107\t Accuracy 0.8598\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8090\t Accuracy 0.8596\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8089\t Accuracy 0.8600\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8151\t Accuracy 0.8588\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8152\t Accuracy 0.8594\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8171\t Accuracy 0.8592\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8180\t Accuracy 0.8585\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8175\t Average training accuracy 0.8584\n",
      "Epoch [19]\t Average validation loss 0.7464\t Average validation accuracy 0.8984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8710.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXxU5dn/8c81mSyQsCcgEAIoQllFSF2roFWr1L1YatXWquXRqm3tpk8Xa7WL1j52+bXVUlSsttjWFXFfsVpEAVERxBU1gmyCskOS6/fHmUAIM5PJcuZMyPf9es3r7OdcGQ7nmvs+97mPuTsiIiINxaIOQEREcpMShIiIJKUEISIiSSlBiIhIUkoQIiKSVDzqAJqqtLTUBwwYEHUYIiJtyrx581a7e1lTtmlzCWLAgAHMnTs36jBERNoUM3u3qduoiklERJJSghARkaSUIEREJKk2dw9CRNqf7du3U1VVxZYtW6IOJecVFRVRXl5Ofn5+i/elBCEiOa+qqopOnToxYMAAzCzqcHKWu7NmzRqqqqoYOHBgi/enKiYRyXlbtmyhR48eSg6NMDN69OjRaiWt0BKEmfUzsyfNbLGZvWpm30qyzhlm9nLi818z2y+seESkbVNyyExrfk9hVjFVA9919/lm1gmYZ2aPuvuieuu8A4xz97VmdhwwBTgwxJhERCRDoZUg3H25u89PjK8HFgN9G6zzX3dfm5h8DigPKx4RkZb4xS9+wfDhwxk1ahSjR49mzpw5nHfeeSxatKjxjVtgwoQJrFu3brf5V1xxBb/5zW9CPXZWblKb2QBgf2BOmtXOBR5Msf1kYDJARUVFs2Ko/PmjrN6wbbf5pSUFzP3x0c3ap4jknjD+r8+ePZuZM2cyf/58CgsLWb16Ndu2bWPq1KktDbdRDzzwQOjHSCX0m9RmVgLcCXzb3T9Jsc4RBAni0mTL3X2Ku1e6e2VZWZO6Etkh2QmTbr6ItE1h/F9fvnw5paWlFBYWAlBaWkqfPn0YP378jq5/brzxRgYPHsz48eP5+te/zkUXXQTA2WefzQUXXMARRxzB3nvvzaxZszjnnHMYOnQoZ5999o5jTJ8+nZEjRzJixAguvXTnpXDAgAGsXr0aCEoxQ4YM4aijjmLJkiXN/nsyFWoJwszyCZLD3939rhTrjAKmAse5+5ow4xGRtu9n973KomVJf2s2atJfZiedP6xPZ356wvCU2x1zzDFceeWVDB48mKOOOopJkyYxbty4HcuXLVvGVVddxfz58+nUqRNHHnkk++23s83N2rVreeKJJ5gxYwYnnHACzz77LFOnTuXTn/40CxYsoGfPnlx66aXMmzePbt26ccwxx3DPPfdw8skn79jHvHnzuP3223nxxReprq5mzJgxjB07tlnfQ6bCbMVkwI3AYne/LsU6FcBdwFnu/npYsYiItERJSQnz5s1jypQplJWVMWnSJKZNm7Zj+fPPP8+4cePo3r07+fn5nHbaabtsf8IJJ2BmjBw5kl69ejFy5EhisRjDhw9n6dKlvPDCC4wfP56ysjLi8ThnnHEGTz/99C77+M9//sMpp5xCx44d6dy5MyeeeGLof3eYJYhDgbOAV8xsQWLeD4EKAHe/Abgc6AH8OdE0q9rdK0OMKanaWicWUxM6kbYg3S99gAGX3Z9y2T//5+BmHzcvL4/x48czfvx4Ro4cyS233LJjmbun3bauaioWi+0Yr5uurq4mHs/sUpztpr5htmJ6xt3N3Ue5++jE5wF3vyGRHHD389y9W73lWU8OABf+Yz4bt1ZHcWgRaQOWLFnCG2+8sWN6wYIF9O/ff8f0AQccwKxZs1i7di3V1dXceeedTdr/gQceyKxZs1i9ejU1NTVMnz59lyosgMMPP5y7776bzZs3s379eu67776W/VEZaDddbZSWFCS9SVVckMfDr37IO6s38tevVNKve8cIohOR1pLq/3ppSUGz97lhwwYuvvhi1q1bRzweZ9CgQUyZMoWJEycC0LdvX374wx9y4IEH0qdPH4YNG0aXLl0y3n/v3r351a9+xRFHHIG7M2HCBE466aRd1hkzZgyTJk1i9OjR9O/fn8MOO6zZf0+mrLGiUa6prKz01n5h0KzXV3HxP+YTz4vx5zPGcNDePVp1/yLSMosXL2bo0KFRh5HWhg0bKCkpobq6mlNOOYVzzjmHU045JZJYkn1fZjavqbU06osJGDe4jHsv+gzdOuZz5tQ53Ppck1+8JCLt3BVXXMHo0aMZMWIEAwcO3KUFUlvVbqqYGjOwtJi7LzyUb9++gJ/cs5DFyz/hihOGUxBXDhWRxoX9VHMUdPWrp3NRPn/9SiUXjN+Hf8x5jzOnzmH1hq1RhyUiEgkliAbyYsalx36K339pNC9VreOkPz7Lq8s+jjosEZGsU4JI4aTRfbnj/EOodWfi9bO5/+XlUYckIpJVugeRxsjyLtx70aFccNt8LvzHfL5/Rx6bttXstp46/BORPZFKEI3o2amIf3z9QL5YWZ40OYA6/BORQP3O+/YEKkFkoDCexzVfGMW/5lZFHYqINObafWHjyt3nF/eE77+x+/wmcnfcnVhsz/99vef/ha1ErzsUaSOSJYd08zOwdOlShg4dyje+8Q3GjBnDrbfeysEHH8yYMWM47bTT2LBhw27blJSU7Bi/4447dunau61QCaKVrN24jW7FzX+UX0Qy9OBl8OErzdv25s8nn7/XSDju6rSbLlmyhJtvvpkrr7ySU089lccee4zi4mKuueYarrvuOi6//PLmxZTDlCBayeG/fpLzx+/DOYcOpENBXtThiEgr69+/PwcddBAzZ85k0aJFHHrooQBs27aNgw9ufi+xuUwJoglSdQLWrWM+Y/t349qHl/C32Uu55KjBTBxbTjxPNXgira6RX/pckaaTvK+l7gq8McXFxUBwD+Loo49m+vTpadevXy29ZcuWZh83SkoQTdBYU9bn3/mIqx9czGV3vcLUZ97hB58bwtHDeun+hcge5KCDDuLCCy/kzTffZNCgQWzatImqqioGDx68y3q9evVi8eLFDBkyhLvvvptOnTpFFHHz6SduKzpgYHfuvOAQbjhzLLXuTL51HhNvmM3cpR9FHZpI+1Hcs2nzm6isrIxp06Zx+umnM2rUKA466CBee+213da7+uqrOf744znyyCPp3bt3qxw729Tdd0iqa2r597wqfvvo66xcv5WjhvZi3rsfsXbT9t3W1YN2Ium1he6+c0lrdfetKqaQxPNinH5ABSeP7stNz77DDU+9xfoUb63Tg3YikotCq2Iys35m9qSZLTazV83sW0nWMTP7g5m9aWYvm9mYsOKJSoeCPC48YhCzfnBE1KGIiDRJmPcgqoHvuvtQ4CDgQjMb1mCd44B9E5/JwPUhxhOp7o08I1Fb27aq+kSyra1Vh0elNb+n0BKEuy939/mJ8fXAYqBvg9VOAv7mgeeArmbWNu/mtND43zzF9U+9pfdPiCRRVFTEmjVrlCQa4e6sWbOGoqKiVtlfVu5BmNkAYH9gToNFfYH3601XJebt0re2mU0mKGFQUVERVpiR6t2liGseeo3rHl3CsSN6c+aBFRwwsLuayIoA5eXlVFVVsWrVqqhDyXlFRUWUl5e3yr5CTxBmVgLcCXzb3T9puDjJJrv9RHD3KcAUCFoxtXqQWZLqQbvSkgL++T8H8+bK9fx9znvcOa+K+15axqCeJZxxYAWnjimnS4f8CCIWyQ35+fkMHDgw6jDanVCbuZpZPjATeNjdr0uy/C/AU+4+PTG9BBjv7infztNWmrm2xOZtNcx8eRm3zXmPl95fR1F+jBNG9eGMg/pz3i0vpEwyaiorIqnkVDNXC+pGbgQWJ0sOCTOAi8zsduBA4ON0yaG96FCQx2mV/Titsh8LP/iYv895j3sXfMC/56XublxNZUWktYVZxXQocBbwipktSMz7IVAB4O43AA8AE4A3gU3A10KMp00a0bcLvzp1JD+c8CnuWbCMn9yzMOqQRKSdCC1BuPszJL/HUH8dBy4MK4Y9SaeifM46qH/aBHHX/CqOGNJT3Y6LSKvQk9R7kO/86yXyYkZl/24cPawXRw/rRf8exVGHJSJtlBLEHmTGRYfy6KIVPLpoBT+/fzE/v38x+/Ys4ehhvThqWC9Gl3clFjMqf/6obnSLSKOUINqYdE1lR5V3ZVR5V757zBDe/2gTjy0OksVfnn6bPz/1FqUlhRw1tGfKG9q60S0i9ak313bg403beer1lTy6aAWzlqxK2WkgwNKrU7ySUUTatJxq5iq5o0vHfE4a3ZeTRvdlW3Utg3/8YMp1f//YG1QO6Mbofl0pLtTpIdKe6QrQzhTE03e/9bvHX8cd8mLG0N6dGFvRjbEDulPZvxt9unbYsZ7uY4js+ZQgZBcLLj+GBe+vY97Sj5j77lr+Pa+KW2a/C0CfLkWM6d+Nyv7ddB9DpB1QgmiH0t3o7tIhn3GDyxg3uAwI3oz32ofrmZtIGPPeXcvMl9v9w+4i7YJuUkuTLVu3mUOufiLl8r5dOzBkr07s26uEwT07MbhXJwb1LKFDQd6OdVRFJZJdukktWVH/XkQyY/t34/UV63nmjdVsq6kFwAwqundk356dGLJXiaqoRNoAJQhpdX84fX8gqJ5aumYTb6xYz+srNvD6ivW8vmI9Ty1ZmXb7e178gL7dOtC3awd6dS4iL5a8xxaVQmRP1hrnd/19FOw1aGxTY1CCkGZJdx+jTjwvxqCeJQzqWcJxI3eu01hT22//c8GO8XjM6N21iL5dO9C3a0f6dutAedcOlHfr0CqlECUZCUtLz63WOL9bWiJXgpBmacnFs7Gmto99ZxwfrNtM1dpNfLB2Mx+s28wHazfz37dW8+EnW8jkttnU/7xNaUkh3YsL6FFSQGlJId06Fux2bCWZPU9r//Ju7j4yPbdqa52t1bVs3l7Dlu01O4bp/P6xN3auv62GLdXBsP4+Nm9Lv49MKEFIzqkrdSSzrbqWDz/eQtW6TXz5rw3fYLvTz+9fnHR+56I4pSWF9CgpoHsjvd6+t2YTJUVxSgrjaZNaS5NMrlzQcmEf2bwwJ+MeXKzT7ePxxSvYWl3Llu01bNkeDHdMV9ewNTEvnYN++fiOi/nW6trG/6gGfvvY6xTkxSjKj9GhII8O+XkU5eftGO9clE9RQR5vrdrY5H3XpwQhkcikiiqZgniMih4dqejRMe16L/30GNZs2MqajdvqDXcdf2d1+v88h1/75C7H7ZxIFnVJo1NRPp0aedr8sUUrKMyPURjPoygxLIzHKMoPhoX5sVCrEnJ1H7W1zvbaWqprnOqanePptn/ytZVsq6llW3Xw2V5Tu3O6Zue8dCb9ZTZbq2sTn+BivmM8sd/GnHtL6laUBfEYRfEYhfl5KdcBOHxw6Y6Let2nQ35sx0W+MJ7H+bfNS7n9m784jnhe+pI4wP0v39/oOukoQUgkwq566dIhny4d8tm7LP16Ay5L/R/o/07bjw1bq1m/ZTvrt1azYUs1GxLD9VuqqVq7mQ1bt6fd/3l/a1mT7PHXPkksZsRjRl4sRjxm9aaNPDPieWlfu8IFaS40mTrrxjnUulNT69R6cIGvcd8xrKkNfn2nM+zyh4KEUFtLbTNa139t2guNrhNP0aChvs4d8oPkHI9REN+ZtAvrJfBrH16Scvt7LjyUovwYRfG8HYm+bhird/x059avJ+7XaJzpZJIcWoMShLRZzS2FZOoLY8szWi/dheC+iz7D1uqgKqLuV2pdtcLW7TVsqa7l6gdfS7n9fv26Ul3r1NTUXYid6trgwlxdW0t1bS1bq9Nfbd9atSGjvyOd9VuqdySkWAzi8Rh5MSNmtmMYM3jtw/Up93H6ARXE84z8WCwY5gUJL54XIz/PiMdi/PDuV1Juf/c3DqEgHqMgL7iw5yeGdfPy84KY0v17/PN/Ds7o702XIEb365rRPlqqNc7vVPvIVJjvpL4JOB5Y6e4jkizvAtxG8ArSOPAbd785rHhkz9MapZCwk8zI8i6NrpMuQfz+S/tndJx0F8VHLhnX4n3cc+GhLd7HT44f1uj26RLE/hXdMoohV7T03GqN87v+Puya45tclAyzBDEN+CPwtxTLLwQWufsJZlYGLDGzv7u7npSSrGkLSUaaJsxf3k3Zx57Qgi3Md1I/bWYD0q0CdDIzA0qAj4DULyoQyVEtvRDkygUtF/aRKxfmPeHi3hpC7YspkSBmpqhi6gTMAD4FdAImuXvS8qmZTQYmA1RUVIx99913wwpZRGSP1Jy+mLJzKzy5zwELgD7AaOCPZtY52YruPsXdK929sqyskWYpIiLSKqJMEF8D7vLAm8A7BKUJERHJAVEmiPeAzwKYWS9gCPB2hPGIiEg9YTZznQ6MB0rNrAr4KZAP4O43AFcB08zsFcCAS919dVjxiIhI04TZiun0RpYvA44J6/giItIyUVYxiYhIDlOCEBGRpJQgREQkKSUIERFJSglCRESSUoIQEZGklCBERCQpJQgREUlKCUJERJJSghARkaSUIEREJCklCBERSUoJQkREklKCEBGRpELr7jvnXLsvbFy5+/zinvD9N7Ifj4hIjms/JYhkySHdfBGRdq79JAgREWmS0BKEmd1kZivNbGGadcab2QIze9XMZoUVi4iINF2YJYhpwLGpFppZV+DPwInuPhw4LcRYRESkiUJLEO7+NPBRmlW+DNzl7u8l1tfNABGRHBLlPYjBQDcze8rM5pnZV0I9WnHPps0XEWnnomzmGgfGAp8FOgCzzew5d3+94YpmNhmYDFBRUdG8o9VvyvrubLj5WDhlCuw3qXn7ExHZw0VZgqgCHnL3je6+Gnga2C/Ziu4+xd0r3b2yrKys5UfudyB0LoeFd7R8XyIie6goE8S9wGFmFjezjsCBwOKsHDkWg5FfgLeegI1rsnJIEZG2JsxmrtOB2cAQM6sys3PN7HwzOx/A3RcDDwEvA88DU909ZZPYVjdiItRWw6J7snZIEZG2JLR7EO5+egbrXAtcG1YMae01EkqHwMI74dPnRhKCiEgua79PUpvByInw7rPwcVXU0YiI5Jz2myAARnwhGC68K9o4RERyUPtOED32gT5j1JpJRCSJ9p0gIKhmWv4SrFaX3yIi9SlBDD8VMHhFpQgRkfoyShBmto+ZFSbGx5vZNxOd7bV9nXvDgM/AK/8G96ijERHJGZmWIO4EasxsEHAjMBD4R2hRZdvIifDRW7B8QdSRiIjkjEwTRK27VwOnAL9z90uA3uGFlWVDT4RYvqqZRETqyTRBbDez04GvAjMT8/LDCSkCHbvDoKOC5q61tVFHIyKSEzJNEF8DDgZ+4e7vmNlA4LbwworAyImwfhm899+oIxERyQkZdbXh7ouAbwKYWTegk7tfHWZgWTfkOMjvGNysHvCZqKMREYlcpq2YnjKzzmbWHXgJuNnMrgs3tCwrKIYhE2DRvVC9LepoREQil2kVUxd3/wQ4FbjZ3ccCR4UXVkRGngab18LbT0YdiYhI5DJNEHEz6w18kZ03qfc8+xwJRV2DaiYRkXYu0wRxJfAw8Ja7v2BmewN7Xt8U8QIYfjK89gBs2xh1NCIikcooQbj7v919lLtfkJh+292/EG5oERkxEbZvhCUPRh2JiEikMr1JXW5md5vZSjNbYWZ3mll52MFFov8h0Kl38CIhEZF2LNMqppuBGUAfoC9wX2LenieWF7wn4o1HgxvWIiLtVKYJoszdb3b36sRnGlCWbgMzuylR4kj7nmkz+7SZ1ZjZxAxjCd+IL0Dtdlg0I+pIREQik2mCWG1mZ5pZXuJzJrCmkW2mAcemW8HM8oBrCG6A544++0P3ffQiIRFp1zJNEOcQNHH9EFgOTCTofiMld38a+KiR/V5M0FPsygzjyI6691W/8x/4ZHnU0YiIRCLTVkzvufuJ7l7m7j3d/WSCh+aazcz6EvQOe0MG6042s7lmNnfVqlUtOWzmRkwEHF69OzvHExHJMS15o9x3Wnjs3wGXuntNYyu6+xR3r3T3yrKytLc+Wk/ZYNhrlB6aE5F2qyUJwlp47ErgdjNbSlBl9WczO7mF+2xdIyfCsvmw5q2oIxERybqWJIgWvZ/T3Qe6+wB3HwDcAXzD3e9pyT5b3YjEs4AL74o2DhGRCKTt7tvM1pM8ERjQoZFtpwPjgVIzqwJ+SuIlQ+7e6H2HnNClHCoOCaqZDv9ecPNaRKSdSJsg3L1Tc3fs7qc3Yd2zm3uc0I2cCPd/B1YshL1GRh2NiEjWtKSKqX0YdjLE4rpZLSLtjhJEY4p7wN5H6H3VItLuKEFkYuRp8PH7UPV81JGIiGSNEkQmPjUB4kWqZhKRdkUJIhOFnWDIcfDqPVCzPepoRESyQgkiUyMmwqbV8PasqCMREckKJYhM7Xs0FHZRD68i0m4oQWQqXgjDToDFM2H75qijEREJnRJEU4yYCNvWw+u59foKEZEwpH2SWhq4a3Iw/PdXoX6DpuKe8P03IglJRCQsKkE0xcYU7zVKNV9EpA1TghARkaSUIEREJCklCBERSUoJQkREklKCaIrinsnndyzNbhwiIlmgZq5N0bAp68rFcMNhsPe4aOIREQlRaCUIM7vJzFaa2cIUy88ws5cTn/+a2X5hxRKankPh8O/DwjvhtQeijkZEpFWFWcU0DTg2zfJ3gHHuPgq4CpgSYizh+cwl0HM4zLwENq+LOhoRkVYTWoJw96eBj9Is/6+7r01MPgeUhxVLqOIFcPKfYOMqeOTHUUcjItJqcuUm9bnAg6kWmtlkM5trZnNXrVqVxbAy1Gd/OORiePFWeOvJqKMREWkVkScIMzuCIEFcmmodd5/i7pXuXllWVpa94Jpi/GXQYxDc903YuiHqaEREWizSBGFmo4CpwEnuvibKWFosvwOc+EdY9z48cVXU0YiItFhkCcLMKoC7gLPc/fWo4mhV/Q+GA74Oc/4C7z0XdTQiIi0SZjPX6cBsYIiZVZnZuWZ2vpmdn1jlcqAH8GczW2Bmc8OKJas++1Po0g/uvQi2b4k6GhGRZgvtQTl3P72R5ecB54V1/MgUlsAJv4PbToVZ18BRP406IhGRZon8JvUeadBnYfSZ8OzvYdmCqKMREWkWJYiwfO7nUFwaVDXVbI86GhGRJlOCCEuHbvD562DFK/DM76KORkSkyZQgwjT0eBh+Cjz9a1j5WtTRiIg0iRJE2I67FgpK4N4LobYm6mhERDKmBBG2kjI47hr4YC7MuSHqaEREMqYEkQ0jT4PBx8LjV8FHb0cdjYhIRpQgssEMjv8t5OXDjG+Ce9QRiYg0Sm+Uy5bOfQCHpf+Bn3XddVlxz93fViciEjGVILJp6/rk8zeuzG4cIiIZUIIQEZGklCBERCQpJQgREUlKCSJXbPkk6ghERHahBJFNxT1TL5v2edigm9UikjvUzDWbUjVlfeNR+NdX4MZj4Ky7ofvA7MYlIpKEShC5YN+j4SszYMs6uOlz8OErUUckIqIEkTP6fRrOeRhicbh5Aix9JuqIRKSdC/Od1DeZ2UozW5hiuZnZH8zsTTN72czGhBVLm1E2BM59BDr1hltPhcUzo45IRNqxMEsQ04Bj0yw/Dtg38ZkMXB9iLG1Hl3I45yHYayT86yyYd0vUEYlIOxVagnD3p4GP0qxyEvA3DzwHdDWz3mHF06Z07A5fnQH7HAn3fROe/o06+BORrIvyHkRf4P1601WJebsxs8lmNtfM5q5atSorwUWuoBhOvx1GfhGeuAoeugxqa6OOSkTakSibuVqSeUl/Jrv7FGAKQGVlZfv5KZ2XD6f8BYrL4Lk/wcbVcPL1EC+IOjIRaQeiTBBVQL960+XAsohiyV2xGHzuF8Gb6R67AjZ/BF+8FQpLoo5MRPZwUSaIGcBFZnY7cCDwsbsvjzCe3GUGn7kEOpbCjIvgV0lq4vROCRFpZWE2c50OzAaGmFmVmZ1rZueb2fmJVR4A3gbeBP4KfCOsWPYYY85KvUzvlBCRVhZaCcLdT29kuQMXhnV8ERFpGT1JLSIiSSlB7EnuOAc+roo6ChHZQyhB7Eleux/+XyU8dQ1s3xx1NCLSxilBtDWp3ilR3BMufB4Gfw6e+iX88QB49R49gS0izWbexi4glZWVPnfu3KjDyG3v/Cd48nrFQhhwGBx7New1IuqoRCRCZjbP3Subso1KEHuigYfB5Fnw+etgxavwl8Ng5iWwcU3UkYlIG6I3yu2p8uLw6XNh+Cnw1NXwwlRYeCcc8SN4+lrYmKRPKz1sJyL1qASxp+vYHSb8Gi54FvrsDw/+IHlyAD1sJyK7UIJoL3oOhbPugUl/jzoSEWkjlCDaEzMYenzUUYhIG6EEIbu690J4b46ax4qIblJLAwvvhhdvg9IhsP+ZsN/pQVfjItLuqATRHqV72O57S+DE/wdFXeDRn8B1n4J/ngmvPwK1NdmNU0QipQflJLWVr8GLt8JLt8Om1dCpD4z+clCyuPGY5K2e1FRWJCc150E5JQhpXPU2eP2hIFm8+Rh4I+/GvuLj7MQlIhlrToLQPQhpXLwAhp0YfD7+AF76Bzzx86ijEpGQ6R6ENE2XvnD499OvM+Pi4KntjauzE5OIhCLUEoSZHQv8HsgDprr71Q2WVwC3AF0T61zm7g+EGZNkwav3wvy/BeO9RsLe42Dv8VBxMBSWBPOv3Vf3MERyXGgJwszygD8BRwNVwAtmNsPdF9Vb7cfAv9z9ejMbRvCe6gFhxSRZ8oO3YfkCePup4PP8FJj9R4jFofyAIGGk6tZD3X2I5IwwSxAHAG+6+9sAZnY7cBJQP0E40Dkx3gVYFmI80pqKe6YuAeTFobwy+Bz+Pdi2Cd5/Dt6eFSSMp67efTsRyTlhJoi+wPv1pquAAxuscwXwiJldDBQDRyXbkZlNBiYDVFRUtHqg0gxNqQYq6Aj7HBl8ADZ9BL8emHr9274AvUYkPsOhdF/Iy999PVVTiYQqzARhSeY1bFN7OjDN3f/PzA4GbjWzEe67tqN09ynAFAiauYYSrWRPx+7pl69fEZQ2arcH03kFUDZkZ8LoNTy4t6FqKpFQhZkgqoB+9abL2b0K6VzgWAB3n21mRUApoP/h7dkFzwTPXlItXjEAAAp2SURBVKx5I3jh0YqFwfDtp+Cl6a17LJVCRFIKM0G8AOxrZgOBD4AvAV9usM57wGeBaWY2FCgCUrysQPYo6e5hQPDsRV1pgS/uXL5xzc6E8fD/pt7/dcOgSz/oWgFdE8Mu/aBrf+hSDvlFif2pFCKSSmgJwt2rzewi4GGCJqw3ufurZnYlMNfdZwDfBf5qZpcQVD+d7W3t0W5pnub+Oi/ukWg2Oy59gth7PKx7D96fEzyT4Q36kSrpFSSMdNyDLtIbo1KI5Kp65+bY3rGxTd081OcgEs80PNBg3uX1xhcBh4YZg7RTJ/9553hNNaxfDh+/HySNde/DuneD6XSuKoWOpVBcCh17JIYNpovLWl4KaY0EoyS1U658ny3dR2vE0MKSsLrakLarsWqqOnnxRDVTP+h/yK7LruiSev+HXBw8Db5pTTBc9mJQxbW1CX1N3XoqFHZK/ikogcLOrVPN1Rr72NMvitn+PpuyD3eorYaa7cGwtjr99u+/EDTiqNkeDGtrdo7XJLava+TRAkoQ0naF/cv4qCuSz6/eFiSNTauD93vfekrqfWxZF5RUtm6Areth2/qmxfDLcogXQrwouG8SL0pMd9g5P16Yfh/P/C54SDEWD5Jl3XgsH2J5ifn56S9I780JqtssBlhivOF0LP0+ViwCPPEyqkRNcsPxdNsvfTaoKvTa4FNbu3O8/vx0nv9rcDH1msRFtKbedGJew+rIhu44p9621fU+DabTubr/rgmhsWM2dGPSJwJanRKEtG+ZlkLqixdA597BpzFff2LX6dpa2FaXLBLDqZ9Nvf2Yr0D1ZqjeCtVbYPuWYFi9FbZ8DNUrgul0Hvtp43E25qZjWr6P6w9u2fbTJrQ8hge+l355LA6Wl36d5S/VS7J59cYTiTa/QzCezqhJDRJ2fr3pxPhDl6be/ow7dh6vbvv6+6rbz+9GpI+jEUoQ0r5lu34+FoOizsEnE8f+MrP10lWV/XB5g1+6Daoy6j5/OTz1Ps68M/FD3xO/1BPD+tM4/Osrqfdx2i3B0IwdpY5gxs7x2xs2dKznKzOCUkosLxhaLLiY15Ve6uZff0jqfXzvzcRFPS/Ytu4Cb3nBv02ddN/nxfNSL6sv3T4m/Lrx7dMliH2PziyGFlKCEGmp5pRCsqmgY8v3MagVqjSGn9yy7fce1/IY2tvrc1OdmxlSghBpqZaWQlojweR6ksqmXPk+W7qP1oih3rk572eWYdFnJyUIkai1RjVXa+xjD7woNlsu7CMHmifrlaMiIu1Ac145qjfKiYhIUkoQIiKSlBKEiIgkpQQhIiJJKUGIiEhSShAiIpKUEoSIiCSlBCEiIkm1uQflzGw9sCTqOAjenb1aMQC5EUcuxAC5EUcuxAC5EUcuxAC5EccQd+/UlA3aYlcbS5r6NGAYzGxu1HHkQgy5EkcuxJArceRCDLkSRy7EkCtxmFmTu6BQFZOIiCSlBCEiIkm1xQQxJeoAEnIhjlyIAXIjjlyIAXIjjlyIAXIjjlyIAXIjjibH0OZuUouISHa0xRKEiIhkgRKEiIgk1aYShJkda2ZLzOxNM7ssguP3M7MnzWyxmb1qZt/Kdgz1YskzsxfNbGaEMXQ1szvM7LXEd3JwRHFckvj3WGhm082sKAvHvMnMVprZwnrzupvZo2b2RmLYLaI4rk38m7xsZnebWddsx1Bv2ffMzM2sNMwY0sVhZhcnrhuvmtmvsx2DmY02s+fMbIGZzTWzA0KOIel1qlnnp7u3iQ+QB7wF7A0UAC8Bw7IcQ29gTGK8E/B6tmOoF8t3gH8AMyP8N7kFOC8xXgB0jSCGvsA7QIfE9L+As7Nw3MOBMcDCevN+DVyWGL8MuCaiOI4B4onxa8KOI1kMifn9gIeBd4HSiL6LI4DHgMLEdM8IYngEOC4xPgF4KuQYkl6nmnN+tqUSxAHAm+7+trtvA24HTspmAO6+3N3nJ8bXA4sJLlBZZWblwOeBqdk+dr0YOhP8Z7gRwN23ufu6iMKJAx3MLA50BJaFfUB3fxr4qMHskwiSJonhyVHE4e6PuHt1YvI5oDzbMST8FvgBkJWWMCniuAC42t23JtZJ8sLr0GNwoHNivAshn59prlNNPj/bUoLoC7xfb7qKCC7OdcxsALA/MCeCw/+O4D9ebQTHrrM3sAq4OVHVNdXMirMdhLt/APwGeA9YDnzs7o9kO46EXu6+PBHXcqBnRHHUdw7wYLYPamYnAh+4+0vZPnYDg4HDzGyOmc0ys09HEMO3gWvN7H2Cc/V/s3XgBtepJp+fbSlBWJJ5kbTRNbMS4E7g2+7+SZaPfTyw0t3nZfO4ScQJitLXu/v+wEaCYmtWJepRTwIGAn2AYjM7M9tx5CIz+xFQDfw9y8ftCPwIuDybx00hDnQDDgK+D/zLzJJdS8J0AXCJu/cDLiFR6g5ba1yn2lKCqCKo06xTThaqEhoys3yCL/3v7n5Xto8PHAqcaGZLCarZjjSz2yKIowqocve6EtQdBAkj244C3nH3Ve6+HbgLOCSCOABWmFlvgMQw1OqMdMzsq8DxwBmeqHTOon0IEvZLifO0HJhvZntlOQ4IztO7PPA8Qak79BvmDXyV4LwE+DdBdXmoUlynmnx+tqUE8QKwr5kNNLMC4EvAjGwGkPjlcSOw2N2vy+ax67j7/7p7ubsPIPgOnnD3rP9idvcPgffNbEhi1meBRdmOg6Bq6SAz65j49/ksQZ1rFGYQXAxIDO+NIggzOxa4FDjR3Tdl+/ju/oq793T3AYnztIrgpumH2Y4FuAc4EsDMBhM0psh2r6rLgHGJ8SOBN8I8WJrrVNPPzzDvpodwd34CwR35t4AfRXD8zxBUa70MLEh8JkT4fYwn2lZMo4G5ie/jHqBbRHH8DHgNWAjcSqLFSsjHnE5wz2M7wQXwXKAH8DjBBeBxoHtEcbxJcL+u7hy9IdsxNFi+lOy0Ykr2XRQAtyXOjfnAkRHE8BlgHkHLyznA2JBjSHqdas75qa42REQkqbZUxSQiIlmkBCEiIkkpQYiISFJKECIikpQShIiIJKUEIdKAmdUket6s+7TaE+JmNiBZr6ciuSgedQAiOWizu4+OOgiRqKkEIZIhM1tqZteY2fOJz6DE/P5m9nji/QuPm1lFYn6vxPsYXkp86roAyTOzvyb66n/EzDpE9keJpKEEIbK7Dg2qmCbVW/aJux8A/JGgV10S439z91EEHeP9ITH/D8Asd9+PoJ+qVxPz9wX+5O7DgXXAF0L+e0SaRU9SizRgZhvcvSTJ/KUEXTW8negM7UN372Fmq4He7r49MX+5u5ea2Sqg3BPvIkjsYwDwqLvvm5i+FMh395+H/5eJNI1KECJN4ynGU62TzNZ64zXoXqDkKCUIkaaZVG84OzH+X4KedQHOAJ5JjD9O8C6AuneI171VTKRN0C8Xkd11MLMF9aYfcve6pq6FZjaH4MfV6Yl53wRuMrPvE7xl72uJ+d8CppjZuQQlhQsIevoUaRN0D0IkQ4l7EJXunu33CYhEQlVMIiKSlEoQIiKSlEoQIiKSlBKEiIgkpQQhIiJJKUGIiEhSShAiIpLU/wdHHJObY0xghQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework2-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "relu_relu_soft_MLP = Network()\n",
    "\n",
    "relu_relu_soft_MLP.add(FCLayer(784, 128))\n",
    "relu_relu_soft_MLP.add(ReLULayer())\n",
    "relu_relu_soft_MLP.add(FCLayer(128, 128))\n",
    "relu_relu_soft_MLP.add(ReLULayer())\n",
    "relu_relu_soft_MLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4801\t Accuracy 0.1000\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.3863\t Accuracy 0.1061\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.3323\t Accuracy 0.1156\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 2.2837\t Accuracy 0.1336\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 2.2436\t Accuracy 0.1556\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 2.2086\t Accuracy 0.1796\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 2.1767\t Accuracy 0.2014\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 2.1500\t Accuracy 0.2223\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 2.1199\t Accuracy 0.2452\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 2.0927\t Accuracy 0.2670\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 2.0672\t Accuracy 0.2864\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0418\t Average training accuracy 0.3065\n",
      "Epoch [0]\t Average validation loss 1.7352\t Average validation accuracy 0.5494\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 1.7474\t Accuracy 0.5500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 1.7347\t Accuracy 0.5524\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 1.7129\t Accuracy 0.5582\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 1.6953\t Accuracy 0.5630\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 1.6794\t Accuracy 0.5746\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 1.6617\t Accuracy 0.5824\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 1.6445\t Accuracy 0.5902\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 1.6338\t Accuracy 0.5950\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 1.6162\t Accuracy 0.6021\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 1.6015\t Accuracy 0.6078\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 1.5870\t Accuracy 0.6143\n",
      "\n",
      "Epoch [1]\t Average training loss 1.5712\t Average training accuracy 0.6211\n",
      "Epoch [1]\t Average validation loss 1.3561\t Average validation accuracy 0.7312\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 1.3544\t Accuracy 0.7600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 1.3745\t Accuracy 0.7012\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 1.3601\t Accuracy 0.7051\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 1.3532\t Accuracy 0.7049\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 1.3455\t Accuracy 0.7079\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 1.3339\t Accuracy 0.7103\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 1.3228\t Accuracy 0.7136\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 1.3194\t Accuracy 0.7139\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 1.3084\t Accuracy 0.7175\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 1.3002\t Accuracy 0.7192\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 1.2917\t Accuracy 0.7216\n",
      "\n",
      "Epoch [2]\t Average training loss 1.2816\t Average training accuracy 0.7248\n",
      "Epoch [2]\t Average validation loss 1.1193\t Average validation accuracy 0.7952\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 1.1193\t Accuracy 0.7900\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 1.1505\t Accuracy 0.7633\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 1.1424\t Accuracy 0.7637\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 1.1425\t Accuracy 0.7613\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 1.1400\t Accuracy 0.7640\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 1.1324\t Accuracy 0.7653\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 1.1258\t Accuracy 0.7669\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 1.1267\t Accuracy 0.7670\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 1.1200\t Accuracy 0.7687\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 1.1158\t Accuracy 0.7691\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 1.1112\t Accuracy 0.7701\n",
      "\n",
      "Epoch [3]\t Average training loss 1.1047\t Average training accuracy 0.7717\n",
      "Epoch [3]\t Average validation loss 0.9765\t Average validation accuracy 0.8262\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.9856\t Accuracy 0.8000\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 1.0157\t Accuracy 0.7922\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 1.0121\t Accuracy 0.7953\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 1.0159\t Accuracy 0.7913\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 1.0160\t Accuracy 0.7933\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 1.0107\t Accuracy 0.7936\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 1.0067\t Accuracy 0.7949\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 1.0099\t Accuracy 0.7944\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 1.0057\t Accuracy 0.7957\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 1.0037\t Accuracy 0.7960\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 1.0012\t Accuracy 0.7966\n",
      "\n",
      "Epoch [4]\t Average training loss 0.9968\t Average training accuracy 0.7979\n",
      "Epoch [4]\t Average validation loss 0.8888\t Average validation accuracy 0.8458\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.9068\t Accuracy 0.8400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.9325\t Accuracy 0.8139\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.9316\t Accuracy 0.8148\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.9374\t Accuracy 0.8096\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.9390\t Accuracy 0.8111\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.9350\t Accuracy 0.8116\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.9325\t Accuracy 0.8121\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.9368\t Accuracy 0.8115\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.9341\t Accuracy 0.8125\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.9334\t Accuracy 0.8130\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.9323\t Accuracy 0.8136\n",
      "\n",
      "Epoch [5]\t Average training loss 0.9292\t Average training accuracy 0.8143\n",
      "Epoch [5]\t Average validation loss 0.8336\t Average validation accuracy 0.8580\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.8595\t Accuracy 0.8500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.8796\t Accuracy 0.8261\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.8808\t Accuracy 0.8280\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.8875\t Accuracy 0.8225\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.8900\t Accuracy 0.8233\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.8867\t Accuracy 0.8238\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.8851\t Accuracy 0.8249\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.8901\t Accuracy 0.8243\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.8883\t Accuracy 0.8251\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.8884\t Accuracy 0.8251\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.8880\t Accuracy 0.8252\n",
      "\n",
      "Epoch [6]\t Average training loss 0.8858\t Average training accuracy 0.8257\n",
      "Epoch [6]\t Average validation loss 0.7982\t Average validation accuracy 0.8656\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.8304\t Accuracy 0.8500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.8458\t Accuracy 0.8367\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.8481\t Accuracy 0.8368\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.8555\t Accuracy 0.8321\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.8584\t Accuracy 0.8326\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.8556\t Accuracy 0.8331\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.8546\t Accuracy 0.8338\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.8600\t Accuracy 0.8334\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.8588\t Accuracy 0.8343\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.8593\t Accuracy 0.8340\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.8594\t Accuracy 0.8338\n",
      "\n",
      "Epoch [7]\t Average training loss 0.8578\t Average training accuracy 0.8342\n",
      "Epoch [7]\t Average validation loss 0.7757\t Average validation accuracy 0.8748\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.8121\t Accuracy 0.8500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.8241\t Accuracy 0.8437\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.8272\t Accuracy 0.8424\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.8349\t Accuracy 0.8374\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.8381\t Accuracy 0.8385\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.8356\t Accuracy 0.8392\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.8349\t Accuracy 0.8399\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.8405\t Accuracy 0.8392\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.8397\t Accuracy 0.8400\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.8406\t Accuracy 0.8397\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.8411\t Accuracy 0.8394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.8398\t Average training accuracy 0.8395\n",
      "Epoch [8]\t Average validation loss 0.7619\t Average validation accuracy 0.8796\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.8016\t Accuracy 0.8600\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.8105\t Accuracy 0.8473\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.8142\t Accuracy 0.8463\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.8220\t Accuracy 0.8423\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.8254\t Accuracy 0.8434\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.8231\t Accuracy 0.8441\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.8227\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.8284\t Accuracy 0.8437\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.8279\t Accuracy 0.8446\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.8290\t Accuracy 0.8447\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.8297\t Accuracy 0.8443\n",
      "\n",
      "Epoch [9]\t Average training loss 0.8288\t Average training accuracy 0.8443\n",
      "Epoch [9]\t Average validation loss 0.7539\t Average validation accuracy 0.8834\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.7960\t Accuracy 0.8600\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.8025\t Accuracy 0.8543\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.8066\t Accuracy 0.8519\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.8145\t Accuracy 0.8472\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.8180\t Accuracy 0.8476\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.8159\t Accuracy 0.8484\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.8156\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.8215\t Accuracy 0.8479\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.8211\t Accuracy 0.8487\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.8224\t Accuracy 0.8488\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.8233\t Accuracy 0.8482\n",
      "\n",
      "Epoch [10]\t Average training loss 0.8225\t Average training accuracy 0.8479\n",
      "Epoch [10]\t Average validation loss 0.7501\t Average validation accuracy 0.8850\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.7934\t Accuracy 0.8800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.7983\t Accuracy 0.8571\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.8027\t Accuracy 0.8551\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.8106\t Accuracy 0.8505\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.8141\t Accuracy 0.8507\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.8121\t Accuracy 0.8518\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.8119\t Accuracy 0.8519\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.8179\t Accuracy 0.8510\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.8177\t Accuracy 0.8517\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.8191\t Accuracy 0.8518\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.8200\t Accuracy 0.8512\n",
      "\n",
      "Epoch [11]\t Average training loss 0.8194\t Average training accuracy 0.8510\n",
      "Epoch [11]\t Average validation loss 0.7487\t Average validation accuracy 0.8878\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.7931\t Accuracy 0.8800\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.7966\t Accuracy 0.8598\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.8011\t Accuracy 0.8575\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.8089\t Accuracy 0.8534\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.8125\t Accuracy 0.8535\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.8106\t Accuracy 0.8543\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.8105\t Accuracy 0.8545\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.8165\t Accuracy 0.8532\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.8163\t Accuracy 0.8538\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.8178\t Accuracy 0.8539\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.8189\t Accuracy 0.8533\n",
      "\n",
      "Epoch [12]\t Average training loss 0.8184\t Average training accuracy 0.8530\n",
      "Epoch [12]\t Average validation loss 0.7494\t Average validation accuracy 0.8894\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.7941\t Accuracy 0.8800\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.7968\t Accuracy 0.8620\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.8013\t Accuracy 0.8588\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.8092\t Accuracy 0.8546\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.8129\t Accuracy 0.8545\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.8110\t Accuracy 0.8553\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.8110\t Accuracy 0.8552\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.8171\t Accuracy 0.8544\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.8170\t Accuracy 0.8550\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.8186\t Accuracy 0.8551\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.8197\t Accuracy 0.8546\n",
      "\n",
      "Epoch [13]\t Average training loss 0.8194\t Average training accuracy 0.8544\n",
      "Epoch [13]\t Average validation loss 0.7520\t Average validation accuracy 0.8900\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.7968\t Accuracy 0.8800\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.7987\t Accuracy 0.8631\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.8034\t Accuracy 0.8605\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.8113\t Accuracy 0.8562\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.8150\t Accuracy 0.8556\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.8132\t Accuracy 0.8565\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.8133\t Accuracy 0.8563\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.8193\t Accuracy 0.8556\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.8194\t Accuracy 0.8562\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.8210\t Accuracy 0.8563\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.8221\t Accuracy 0.8558\n",
      "\n",
      "Epoch [14]\t Average training loss 0.8219\t Average training accuracy 0.8555\n",
      "Epoch [14]\t Average validation loss 0.7558\t Average validation accuracy 0.8890\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.8007\t Accuracy 0.8800\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.8019\t Accuracy 0.8625\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.8067\t Accuracy 0.8605\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.8145\t Accuracy 0.8560\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.8183\t Accuracy 0.8557\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.8165\t Accuracy 0.8567\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.8166\t Accuracy 0.8566\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.8226\t Accuracy 0.8560\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.8227\t Accuracy 0.8566\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.8244\t Accuracy 0.8566\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.8255\t Accuracy 0.8560\n",
      "\n",
      "Epoch [15]\t Average training loss 0.8253\t Average training accuracy 0.8556\n",
      "Epoch [15]\t Average validation loss 0.7604\t Average validation accuracy 0.8902\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.8054\t Accuracy 0.8700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.8058\t Accuracy 0.8627\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.8107\t Accuracy 0.8599\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.8185\t Accuracy 0.8562\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.8223\t Accuracy 0.8557\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.8204\t Accuracy 0.8567\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.8206\t Accuracy 0.8569\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.8266\t Accuracy 0.8563\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.8267\t Accuracy 0.8570\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.8284\t Accuracy 0.8570\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.8295\t Accuracy 0.8563\n",
      "\n",
      "Epoch [16]\t Average training loss 0.8294\t Average training accuracy 0.8559\n",
      "Epoch [16]\t Average validation loss 0.7654\t Average validation accuracy 0.8904\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.8103\t Accuracy 0.8700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.8103\t Accuracy 0.8631\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.8151\t Accuracy 0.8608\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.8228\t Accuracy 0.8571\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.8267\t Accuracy 0.8567\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.8248\t Accuracy 0.8576\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.8250\t Accuracy 0.8576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.8310\t Accuracy 0.8569\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.8311\t Accuracy 0.8577\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.8328\t Accuracy 0.8576\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.8339\t Accuracy 0.8569\n",
      "\n",
      "Epoch [17]\t Average training loss 0.8339\t Average training accuracy 0.8564\n",
      "Epoch [17]\t Average validation loss 0.7707\t Average validation accuracy 0.8898\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.8154\t Accuracy 0.8700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.8149\t Accuracy 0.8629\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.8197\t Accuracy 0.8607\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.8274\t Accuracy 0.8572\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.8314\t Accuracy 0.8567\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.8294\t Accuracy 0.8573\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.8297\t Accuracy 0.8575\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.8356\t Accuracy 0.8568\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.8357\t Accuracy 0.8574\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.8374\t Accuracy 0.8573\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.8385\t Accuracy 0.8567\n",
      "\n",
      "Epoch [18]\t Average training loss 0.8385\t Average training accuracy 0.8561\n",
      "Epoch [18]\t Average validation loss 0.7761\t Average validation accuracy 0.8880\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.8206\t Accuracy 0.8600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.8196\t Accuracy 0.8618\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.8245\t Accuracy 0.8599\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.8321\t Accuracy 0.8564\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.8361\t Accuracy 0.8562\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.8341\t Accuracy 0.8567\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.8344\t Accuracy 0.8569\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.8403\t Accuracy 0.8565\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.8404\t Accuracy 0.8569\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.8421\t Accuracy 0.8569\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.8432\t Accuracy 0.8563\n",
      "\n",
      "Epoch [19]\t Average training loss 0.8431\t Average training accuracy 0.8558\n",
      "Epoch [19]\t Average validation loss 0.7814\t Average validation accuracy 0.8880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relu_relu_soft_MLP, relu_relu_soft_loss, relu_relu_soft_acc = train(relu_relu_soft_MLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.8643.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(relu_relu_soft_MLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'relu_relu_soft': [relu_relu_soft_loss, relu_relu_soft_acc]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
