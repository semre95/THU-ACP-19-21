{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (14.2s): Lower bound = -173.15237426757812\n",
      "Epoch 2 (13.1s): Lower bound = -125.95645904541016\n",
      "Epoch 3 (13.1s): Lower bound = -115.4300537109375\n",
      "Epoch 4 (13.3s): Lower bound = -111.14623260498047\n",
      "Epoch 5 (17.0s): Lower bound = -108.410400390625\n",
      "Epoch 6 (18.3s): Lower bound = -106.51114654541016\n",
      "Epoch 7 (18.1s): Lower bound = -105.159423828125\n",
      "Epoch 8 (13.0s): Lower bound = -104.07119750976562\n",
      "Epoch 9 (13.5s): Lower bound = -103.14167022705078\n",
      "Epoch 10 (13.5s): Lower bound = -102.26787567138672\n",
      ">>> TEST (379.8s)\n",
      ">> Test lower bound = -101.43687438964844\n",
      ">> Test log likelihood (IS) = -96.09476470947266\n",
      "Epoch 11 (14.1s): Lower bound = -101.62956237792969\n",
      "Epoch 12 (13.1s): Lower bound = -100.99659729003906\n",
      "Epoch 13 (12.9s): Lower bound = -100.55213165283203\n",
      "Epoch 14 (13.1s): Lower bound = -100.00975036621094\n",
      "Epoch 15 (12.9s): Lower bound = -99.74927520751953\n",
      "Epoch 16 (12.9s): Lower bound = -99.3624267578125\n",
      "Epoch 17 (12.8s): Lower bound = -99.09620666503906\n",
      "Epoch 18 (12.9s): Lower bound = -98.78494262695312\n",
      "Epoch 19 (13.0s): Lower bound = -98.54756164550781\n",
      "Epoch 20 (13.2s): Lower bound = -98.28917694091797\n",
      ">>> TEST (402.6s)\n",
      ">> Test lower bound = -98.56088256835938\n",
      ">> Test log likelihood (IS) = -93.06107330322266\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "import zhusuan as zs\n",
    "\n",
    "from examples import conf\n",
    "from examples.utils import dataset, save_image_collections\n",
    "\n",
    "\n",
    "@zs.meta_bayesian_net(scope=\"gen\", reuse_variables=True)\n",
    "def build_gen(x_dim, z_dim, n, n_particles=1):\n",
    "    bn = zs.BayesianNet()\n",
    "    z_mean = tf.zeros([n, z_dim])\n",
    "    z = bn.normal(\"z\", z_mean, std=1., group_ndims=1, n_samples=n_particles)\n",
    "    h = tf.layers.dense(z, 500, activation=tf.nn.relu)\n",
    "    h = tf.layers.dense(h, 500, activation=tf.nn.relu)\n",
    "    x_logits = tf.layers.dense(h, x_dim)\n",
    "    bn.deterministic(\"x_mean\", tf.sigmoid(x_logits))\n",
    "    bn.bernoulli(\"x\", x_logits, group_ndims=1)\n",
    "    return bn\n",
    "\n",
    "\n",
    "@zs.reuse_variables(scope=\"q_net\")\n",
    "def build_q_net(x, z_dim, n_z_per_x):\n",
    "    bn = zs.BayesianNet()\n",
    "    h = tf.layers.dense(tf.cast(x, tf.float32), 500, activation=tf.nn.relu)\n",
    "    h = tf.layers.dense(h, 500, activation=tf.nn.relu)\n",
    "    z_mean = tf.layers.dense(h, z_dim)\n",
    "    z_logstd = tf.layers.dense(h, z_dim)\n",
    "    bn.normal(\"z\", z_mean, logstd=z_logstd, group_ndims=1, n_samples=n_z_per_x)\n",
    "    return bn\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load MNIST\n",
    "    data_path = os.path.join(conf.data_dir, \"mnist.pkl.gz\")\n",
    "    x_train, t_train, x_valid, t_valid, x_test, t_test = \\\n",
    "        dataset.load_mnist_realval(data_path)\n",
    "    x_train = np.vstack([x_train, x_valid])\n",
    "    x_test = np.random.binomial(1, x_test, size=x_test.shape)\n",
    "    x_dim = x_train.shape[1]\n",
    "\n",
    "    # Define model parameters\n",
    "    z_dim = 40\n",
    "\n",
    "    # Build the computation graph\n",
    "    n_particles = tf.placeholder(tf.int32, shape=[], name=\"n_particles\")\n",
    "    x_input = tf.placeholder(tf.float32, shape=[None, x_dim], name=\"x\")\n",
    "    x = tf.cast(tf.less(tf.random_uniform(tf.shape(x_input)), x_input),\n",
    "                tf.int32)\n",
    "    n = tf.placeholder(tf.int32, shape=[], name=\"n\")\n",
    "\n",
    "    model = build_gen(x_dim, z_dim, n, n_particles)\n",
    "    variational = build_q_net(x, z_dim, n_particles)\n",
    "\n",
    "    lower_bound = zs.variational.elbo(\n",
    "        model, {\"x\": x}, variational=variational, axis=0)\n",
    "    cost = tf.reduce_mean(lower_bound.sgvb())\n",
    "    lower_bound = tf.reduce_mean(lower_bound)\n",
    "\n",
    "    # # Importance sampling estimates of marginal log likelihood\n",
    "    is_log_likelihood = tf.reduce_mean(\n",
    "        zs.is_loglikelihood(model, {\"x\": x}, proposal=variational, axis=0))\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    infer_op = optimizer.minimize(cost)\n",
    "\n",
    "    # Random generation\n",
    "    x_gen = tf.reshape(model.observe()[\"x_mean\"], [-1, 28, 28, 1])\n",
    "\n",
    "    # Define training/evaluation parameters\n",
    "    epochs = 20\n",
    "    batch_size = 128\n",
    "    iters = x_train.shape[0] // batch_size\n",
    "    save_freq = 10\n",
    "    test_freq = 10\n",
    "    test_batch_size = 400\n",
    "    test_iters = x_test.shape[0] // test_batch_size\n",
    "    result_path = \"results/vae\"\n",
    "\n",
    "    # Run the inference\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            time_epoch = -time.time()\n",
    "            np.random.shuffle(x_train)\n",
    "            lbs = []\n",
    "            for t in range(iters):\n",
    "                x_batch = x_train[t * batch_size:(t + 1) * batch_size]\n",
    "                _, lb = sess.run([infer_op, lower_bound],\n",
    "                                 feed_dict={x_input: x_batch,\n",
    "                                            n_particles: 1,\n",
    "                                            n: batch_size})\n",
    "                lbs.append(lb)\n",
    "            time_epoch += time.time()\n",
    "            print(\"Epoch {} ({:.1f}s): Lower bound = {}\".format(\n",
    "                epoch, time_epoch, np.mean(lbs)))\n",
    "\n",
    "            if epoch % test_freq == 0:\n",
    "                time_test = -time.time()\n",
    "                test_lbs, test_lls = [], []\n",
    "                for t in range(test_iters):\n",
    "                    test_x_batch = x_test[t * test_batch_size:\n",
    "                                          (t + 1) * test_batch_size]\n",
    "                    test_lb = sess.run(lower_bound,\n",
    "                                       feed_dict={x: test_x_batch,\n",
    "                                                  n_particles: 1,\n",
    "                                                  n: test_batch_size})\n",
    "                    test_ll = sess.run(is_log_likelihood,\n",
    "                                       feed_dict={x: test_x_batch,\n",
    "                                                  n_particles: 1000,\n",
    "                                                  n: test_batch_size})\n",
    "                    test_lbs.append(test_lb)\n",
    "                    test_lls.append(test_ll)\n",
    "                time_test += time.time()\n",
    "                print(\">>> TEST ({:.1f}s)\".format(time_test))\n",
    "                print(\">> Test lower bound = {}\".format(np.mean(test_lbs)))\n",
    "                print('>> Test log likelihood (IS) = {}'.format(\n",
    "                    np.mean(test_lls)))\n",
    "\n",
    "            if epoch % save_freq == 0:\n",
    "                images = sess.run(x_gen, feed_dict={n: 100, n_particles: 1})\n",
    "                name = os.path.join(result_path,\n",
    "                                    \"vae.epoch.{}.png\".format(epoch))\n",
    "                save_image_collections(images, name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
